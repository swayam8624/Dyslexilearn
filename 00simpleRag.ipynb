{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run a local RAG pipeline from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG ?\n",
    "\n",
    "RAG stands for retrieval augmented Generation.\n",
    "\n",
    "It was introduced in the paper [_Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks_](https://arxiv.org/abs/2005.11401).\n",
    "\n",
    "The goal of RAG is to take information and pass it to an LLM so it can generate outputs based on that information.\n",
    "\n",
    "- **Retrieval** --> Find Relevant information given a query , e.g. \"what are the macronutrients and what do they do?\" --> retrieves passages of the text related to the macronutrients from a nutrition textbook .\n",
    "\n",
    "- **Augmented** --> To take the relevant information and augment out input(prompt) to an LLm with that relevant information\n",
    "\n",
    "- **Generation**--> take result of above two steps and pass them on to a LLM for generative outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why RAG?\n",
    "\n",
    "The main goal of RAG is to improve the generation outputs of LLMs .\n",
    "\n",
    "1. To prevent hallucinations - LLMs are capable of generating _good looking_ texts , but that doesn't mean , it is factually correct , RAG can help LLMs to generate passage based on relevant passages that are factual .\n",
    "\n",
    "2. Work with Custom Data - Many base LLMs are trained with internet-scale data. This means they have a fairly good understanding of language in general , However that also means the responses can be generic in nature , RAG helps generating based on specific data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Local?\n",
    "\n",
    "Fun...\n",
    "\n",
    "Privacy , Speed and Cost\n",
    "\n",
    "- Privacy -- IF you have a private documentation, maybe you dont want to send you information to an API , You want to setup an LLM and run it on your own Hardware.\n",
    "\n",
    "- Speed -- Whenever you use an API , you have to send some kind of data across the internet which takes time. Running Locally means we dont have to wait for transfer of data\n",
    "\n",
    "- Cost -- If You own you own hardware , the cost is paid , no or least operational cost , only Initial cost.\n",
    "\n",
    "- no Vendor Lockin - if API shuts down , you dont have to worry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database.True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document/Text Processing and Embedding Creation\n",
    "\n",
    "Ingredients:\n",
    "\n",
    "- PDF document of choice.\n",
    "- Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Import PDF document.\n",
    "2. Process text for embedding (e.g. split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "\n",
    "#Get pdf path\n",
    "pdf_path = \"./Gift_of_Dyslexia.pdf\"\n",
    "\n",
    "#download pdf if it does not exist \n",
    "\n",
    "# if not os.path.exists(pdf_path):\n",
    "#     print(f\"[INFO] files doesn't exist , downloading...\")\n",
    "\n",
    "#     # The URL of the PDF you want to download\n",
    "#     url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "#     # The local filename to save the downloaded file\n",
    "#     filename = pdf_path\n",
    "\n",
    "#     # Send a GET request to the URL\n",
    "#     response = requests.get(url)\n",
    "\n",
    "#     # Check if the request was successful\n",
    "#     if response.status_code == 200:\n",
    "#         # Open a file in binary write mode and save the content to it\n",
    "#         with open(filename, \"wb\") as file:\n",
    "#             file.write(response.content)\n",
    "#         print(f\"The file has been downloaded and saved as {filename}\")\n",
    "#     else:\n",
    "#         print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "# else:\n",
    "#     print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b04b40e77c04b4abc42e02ec2d28144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'page_char_count': 145,\n",
       "  'page_word_count': 35,\n",
       "  'page_sentence_count_row': 3,\n",
       "  'page_token_count': 36.25,\n",
       "  'text': \"THE GIFT  OF DYSLEXIA  W h y Some of the Smartest People  Can't Read and How They Can Learn  Ronald D. Davis  with Eldon M. Braun  A Perigee Book\"},\n",
       " {'page_number': -40,\n",
       "  'page_char_count': 650,\n",
       "  'page_word_count': 137,\n",
       "  'page_sentence_count_row': 4,\n",
       "  'page_token_count': 162.5,\n",
       "  'text': \"Contents  Foreword by Dr. Joan Smith  Author's  Note  Preface  Acknowledgments  Part One What Dyslexia Really Is  Chapter 1  The Underlying Talent  Chapter 2  The Learning Disability  Chapter 3  Effects of Disorientation  Chapter 4  Dyslexia in Action  Chapter 5  Compulsive Solutions  Chapter 6  Problems with Reading  Chapter 7  Spelling Problems  Chapter 8  Math Problems  Chapter 9  Handwriting Problems  Chapter 10  The Newest Disability: A D D  Chapter 11  Clumsiness  Chapter 12  A Real Solution  Part Two  Little P . D . — A Developmental Theory  of Dyslexia  Chapter 13  How Dyslexia Happens  Chapter 14  The Two-Year-Old and the Kitten  vii\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs minor formatting on texts.\n",
    "    \"\"\"\n",
    "    cleaned_text = text.replace('\\n', \" \" ).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path : str)-> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_text = []\n",
    "    for page_number , page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text = text)\n",
    "        pages_and_text.append({\n",
    "            \"page_number\": page_number -41,\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(text.split(\" \")),\n",
    "            \"page_sentence_count_row\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text)/4,\n",
    "            \"text\":text\n",
    "            })\n",
    "    return pages_and_text\n",
    "\n",
    "pages_and_text = open_and_read_pdf(pdf_path = pdf_path)\n",
    "pages_and_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 68,\n",
       "  'page_char_count': 1462,\n",
       "  'page_word_count': 291,\n",
       "  'page_sentence_count_row': 15,\n",
       "  'page_token_count': 365.5,\n",
       "  'text': \"Understanding  iP^ talent  Pictured thoughts are as thorough or deep as these mental  pictures are accurate in portraying the meanings of the  words that the person would use to describe the same  thoughts.  We could say pictured thoughts are of substance while  verbal thoughts are significant sound.  Intuition  The only drawback to picture thinking is that the person  doing it is not aware of the individual pictures as they  occur. It happens too fast. The incidence of awareness is  the amount of time it takes for something to register  consciously in the awareness of the individual. In humans  it is fairly consistent at V25 of a second. In other words, a  stimulus must be present for V25 of a second in order to  register in the person's consciousness.  If a stimulus is present longer than V25 second, we are  aware of it. This is called cognizance. If a stimulus is  present for less than V25 of a second, but longer than V36 of  a second, it falls into the category called subliminal. Our  brain gets it, but we aren't aware of what it got. If it is  part of a continuum, it fuses with the pictures that precede  and follow. If a stimulus isn't present for at least  l h e of a  second, we don't even get it subliminally. It went by too  fast for our brain to catch it at all.  Picture thinking seems to be consistently happening  at about thirty-two pictures per second, or a frequency  of V32 of a second, the same speed as the flicker-fusion  99\"},\n",
       " {'page_number': 77,\n",
       "  'page_char_count': 1584,\n",
       "  'page_word_count': 289,\n",
       "  'page_sentence_count_row': 19,\n",
       "  'page_token_count': 396.0,\n",
       "  'text': 'The Gift  Conditioning is a very rudimentary form of learning.  When we train a dog or a seal to do tricks, we condition  the animal through rewards or penalties to behave in a  desired manner. Humans can also be conditioned in this  way, but it is much more difficult. Often the conditioning  isn\\'t effective.  Because of creativity, humans learn on a much higher  level. The act of reasoning is a function of creativity. Logic  is a product of creativity. Reasoning and logic are the  foundations of learning. If we touch something that burns  our fingers, it is with reasoning and logic we figure out  why we shouldn\\'t touch it again. We learnednot to touch it.  In chapter 15,1 mentioned that dyslexic children often  haven\\'t developed their skills in reasoning and logic by  the time they start school. What they have developed is a  variation of these skills that does not follow the linear model  of verbal thought. Their analytical reasoning and logic is  comparative, using pictures instead of words. This method  might be great for figuring out the helical structure of  DNA, but could be useless in attempting to do a math story  problem in fifth grade.  Sometimes, like autistic savants, dyslexics can \"see\" the  answers to math problems without using pen and paper.  This is actually a highly developed form of reasoning. They  solved the problem whether or not they bothered to go  through the conventional steps. Often, investigation reveals  that they have developed highly creative mathematical  shortcuts.  If the creative process and the human learning process  108'},\n",
       " {'page_number': 4,\n",
       "  'page_char_count': 878,\n",
       "  'page_word_count': 170,\n",
       "  'page_sentence_count_row': 10,\n",
       "  'page_token_count': 219.5,\n",
       "  'text': 'CHAPTER 6  You may have forgotten what it was like to learn to read.  Most people who can read fairly well do it automatically,  unaware of how many gyrations their minds are going  through. Reading is considered by many researchers to be  the most complex function we require our brains to perform.  Maybe you have heard about computer software that  performs optical character recognition. It \"reads\" an  image of printing and converts it into text characters that  can be used in a computer program. The Postal Service  uses it to read typed zip codes for mail sorting. These  programs take a long time to work as they crawl along,  letter by letter. They also tend to make many mistakes.  It\\'s a wonder they work at all, considering the complexity  of what they have to do.  When you read, your brain has to do the same thing  32  Problems with Reading  (Especially English)'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "random.sample(pages_and_text , k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_row</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>145</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>36.25</td>\n",
       "      <td>THE GIFT  OF DYSLEXIA  W h y Some of the Smart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>650</td>\n",
       "      <td>137</td>\n",
       "      <td>4</td>\n",
       "      <td>162.50</td>\n",
       "      <td>Contents  Foreword by Dr. Joan Smith  Author's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>794</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>198.50</td>\n",
       "      <td>Contents  A g e s Three to Five  The First Day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>974</td>\n",
       "      <td>169</td>\n",
       "      <td>8</td>\n",
       "      <td>243.50</td>\n",
       "      <td>Foreword  During my twenty-five years of exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>1792</td>\n",
       "      <td>301</td>\n",
       "      <td>25</td>\n",
       "      <td>448.00</td>\n",
       "      <td>Foreword  Four different learning locks are op...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_row  \\\n",
       "0          -41              145               35                        3   \n",
       "1          -40              650              137                        4   \n",
       "2          -39              794              171                        1   \n",
       "3          -38              974              169                        8   \n",
       "4          -37             1792              301                       25   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0             36.25  THE GIFT  OF DYSLEXIA  W h y Some of the Smart...  \n",
       "1            162.50  Contents  Foreword by Dr. Joan Smith  Author's...  \n",
       "2            198.50  Contents  A g e s Three to Five  The First Day...  \n",
       "3            243.50  Foreword  During my twenty-five years of exper...  \n",
       "4            448.00  Foreword  Four different learning locks are op...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.DataFrame(pages_and_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_row</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>89.00</td>\n",
       "      <td>1000.72</td>\n",
       "      <td>193.88</td>\n",
       "      <td>11.14</td>\n",
       "      <td>250.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>75.49</td>\n",
       "      <td>440.05</td>\n",
       "      <td>81.40</td>\n",
       "      <td>5.48</td>\n",
       "      <td>110.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.00</td>\n",
       "      <td>770.00</td>\n",
       "      <td>147.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>192.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>89.00</td>\n",
       "      <td>980.00</td>\n",
       "      <td>209.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>245.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>154.00</td>\n",
       "      <td>1359.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>339.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>219.00</td>\n",
       "      <td>2133.00</td>\n",
       "      <td>371.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>533.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_row  \\\n",
       "count       261.00           261.00           261.00                   261.00   \n",
       "mean         89.00          1000.72           193.88                    11.14   \n",
       "std          75.49           440.05            81.40                     5.48   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%          24.00           770.00           147.00                     8.00   \n",
       "50%          89.00           980.00           209.00                    11.00   \n",
       "75%         154.00          1359.00           260.00                    15.00   \n",
       "max         219.00          2133.00           371.00                    25.00   \n",
       "\n",
       "       page_token_count  \n",
       "count            261.00  \n",
       "mean             250.18  \n",
       "std              110.01  \n",
       "min                0.00  \n",
       "25%              192.50  \n",
       "50%              245.00  \n",
       "75%              339.75  \n",
       "max              533.25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks like our average token count per page is 287.\n",
    "\n",
    "For this particular use case, it means we could embed an average whole page with the `all-mpnet-base-v2` model (this model has an input capacity of 384).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another sentence., I like elephants]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create document instance as an example \n",
    "doc = nlp(\"This is a sentence. This another sentence. I like elephants\")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37cdda443d546b1bb9348d98feeea23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_text):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    #Make sure all sentences are strings\n",
    "    \n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    #count the sentences\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': -1,\n",
       "  'page_char_count': 822,\n",
       "  'page_word_count': 142,\n",
       "  'page_sentence_count_row': 8,\n",
       "  'page_token_count': 205.5,\n",
       "  'text': 'CHAPTER 5  Once disorientations begin to cause mistakes, the dyslexic  child becomes frustrated. Nobody likes to make mistakes,  so around the age of nine, in about third grade, the  dyslexic child begins to find, figure out and adopt  solutions to the problem. Even though this may seem like  a good thing, it is actually how the reading problem  becomes a true learning disability.  The solutions dyslexics invent don\\'t solve the real  problem of distorted perceptions; they only afford  temporary relief from frustrations. They are roundabout  methods of coping with the effects of disorientation. They  ultimately slow down the learning process and form the  real learning disability.  These \"solutions\" are methods of doing things and  tactics for knowing or remembering things. They quickly  27  Compulsive Solutions',\n",
       "  'sentences': ['CHAPTER 5  Once disorientations begin to cause mistakes, the dyslexic  child becomes frustrated.',\n",
       "   'Nobody likes to make mistakes,  so around the age of nine, in about third grade, the  dyslexic child begins to find, figure out and adopt  solutions to the problem.',\n",
       "   'Even though this may seem like  a good thing, it is actually how the reading problem  becomes a true learning disability.',\n",
       "   \" The solutions dyslexics invent don't solve the real  problem of distorted perceptions; they only afford  temporary relief from frustrations.\",\n",
       "   'They are roundabout  methods of coping with the effects of disorientation.',\n",
       "   'They  ultimately slow down the learning process and form the  real learning disability.',\n",
       "   ' These \"solutions\" are methods of doing things and  tactics for knowing or remembering things.',\n",
       "   'They quickly  27  Compulsive Solutions'],\n",
       "  'page_sentence_count_spacy': 8}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_text , k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_row</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>89.00</td>\n",
       "      <td>1000.72</td>\n",
       "      <td>193.88</td>\n",
       "      <td>11.14</td>\n",
       "      <td>250.18</td>\n",
       "      <td>11.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>75.49</td>\n",
       "      <td>440.05</td>\n",
       "      <td>81.40</td>\n",
       "      <td>5.48</td>\n",
       "      <td>110.01</td>\n",
       "      <td>5.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.00</td>\n",
       "      <td>770.00</td>\n",
       "      <td>147.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>192.50</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>89.00</td>\n",
       "      <td>980.00</td>\n",
       "      <td>209.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>245.00</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>154.00</td>\n",
       "      <td>1359.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>339.75</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>219.00</td>\n",
       "      <td>2133.00</td>\n",
       "      <td>371.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>533.25</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_row  \\\n",
       "count       261.00           261.00           261.00                   261.00   \n",
       "mean         89.00          1000.72           193.88                    11.14   \n",
       "std          75.49           440.05            81.40                     5.48   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%          24.00           770.00           147.00                     8.00   \n",
       "50%          89.00           980.00           209.00                    11.00   \n",
       "75%         154.00          1359.00           260.00                    15.00   \n",
       "max         219.00          2133.00           371.00                    25.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count            261.00                     261.00  \n",
       "mean             250.18                      11.54  \n",
       "std              110.01                       5.70  \n",
       "min                0.00                       0.00  \n",
       "25%              192.50                       8.00  \n",
       "50%              245.00                      12.00  \n",
       "75%              339.75                      15.00  \n",
       "max              533.25                      25.00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_text)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define split size to turn groups of sentences into chunks\n",
    "\n",
    "num_sentences_chunk_size = 10\n",
    "\n",
    "#Create a function to split the list of text recursively into chunk size\n",
    "\n",
    "def split_list(input_list :list,\n",
    "               split_size:int = num_sentences_chunk_size) -> list[list[str]]:\n",
    "    return [input_list[i:i+split_size] for i in range(0, len(input_list), split_size)]\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e434c9a31b254355a20755a134d3e9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loop through pages and texts and plit sentences into chunks\n",
    "\n",
    "for item in tqdm(pages_and_text):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"])\n",
    "    \n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 62,\n",
       "  'page_char_count': 1560,\n",
       "  'page_word_count': 323,\n",
       "  'page_sentence_count_row': 13,\n",
       "  'page_token_count': 390.0,\n",
       "  'text': 'Little P.D.—A Developmental Theory of Dyslexia  A Discovery  In 1980, I was lucky enough to discover how to correct the  severe perceptual distortions  that had been  my  everyday  reality for  thirty-eight years,  I was working as a sculptor when another artist wrote and  asked me about my sculpting technique. His letter was so  filled with praise  that  I began  the  laborious process  of  composing a response. Hours later, after carefully getting my  thoughts  down,  I  discovered  that  the  letter  was  totally  illegible—-just a bunch of meaningless scrawls  that nobody  could ever read.  Months later, it occurred to me that when / wrote the letter,  I had been focusing on my creative process.  I wondered if this  was what had made my dyslexia worse.  The engineer in me  reasoned that if my dyslexia could be changed by something I  was doing mentally,  it could not possibly be a structural  problem but must be a functional problem.  Thus,  there had to  be something I could do mentally to correct my dyslexia. This  was my first step as a researcher in the field of learning  disabilities.  Three days later, I managed to figure out how to correct my  perceptual distortions.  I  went  to  the  library,  picked  up  Treasure Island, and, for the first time in my life, read a book  cover to cover in just a few hours.  Since then, I have worked at developing techniques based  on what I discovered.  I have had the pleasure of helping  more than  1,500 dyslexic children and adults learn to make  the  words—and the world—stand still.  92',\n",
       "  'sentences': ['Little P.D.—A Developmental Theory of Dyslexia  A Discovery  In 1980, I was lucky enough to discover how to correct the  severe perceptual distortions  that had been  my  everyday  reality for  thirty-eight years,  I was working as a sculptor when another artist wrote and  asked me about my sculpting technique.',\n",
       "   'His letter was so  filled with praise  that  I began  the  laborious process  of  composing a response.',\n",
       "   'Hours later, after carefully getting my  thoughts  down,  I  discovered  that  the  letter  was  totally  illegible—-just a bunch of meaningless scrawls  that nobody  could ever read.',\n",
       "   ' Months later, it occurred to me that when / wrote the letter,  I had been focusing on my creative process.',\n",
       "   ' I wondered if this  was what had made my dyslexia worse.',\n",
       "   ' The engineer in me  reasoned that if my dyslexia could be changed by something I  was doing mentally,  it could not possibly be a structural  problem but must be a functional problem.',\n",
       "   ' Thus,  there had to  be something I could do mentally to correct my dyslexia.',\n",
       "   'This  was my first step as a researcher in the field of learning  disabilities.',\n",
       "   ' Three days later, I managed to figure out how to correct my  perceptual distortions.',\n",
       "   ' I  went  to  the  library,  picked  up  Treasure Island, and, for the first time in my life, read a book  cover to cover in just a few hours.',\n",
       "   ' Since then, I have worked at developing techniques based  on what I discovered.',\n",
       "   ' I have had the pleasure of helping  more than  1,500 dyslexic children and adults learn to make  the  words—and the world—stand still.',\n",
       "   ' 92'],\n",
       "  'page_sentence_count_spacy': 13,\n",
       "  'sentence_chunks': [['Little P.D.—A Developmental Theory of Dyslexia  A Discovery  In 1980, I was lucky enough to discover how to correct the  severe perceptual distortions  that had been  my  everyday  reality for  thirty-eight years,  I was working as a sculptor when another artist wrote and  asked me about my sculpting technique.',\n",
       "    'His letter was so  filled with praise  that  I began  the  laborious process  of  composing a response.',\n",
       "    'Hours later, after carefully getting my  thoughts  down,  I  discovered  that  the  letter  was  totally  illegible—-just a bunch of meaningless scrawls  that nobody  could ever read.',\n",
       "    ' Months later, it occurred to me that when / wrote the letter,  I had been focusing on my creative process.',\n",
       "    ' I wondered if this  was what had made my dyslexia worse.',\n",
       "    ' The engineer in me  reasoned that if my dyslexia could be changed by something I  was doing mentally,  it could not possibly be a structural  problem but must be a functional problem.',\n",
       "    ' Thus,  there had to  be something I could do mentally to correct my dyslexia.',\n",
       "    'This  was my first step as a researcher in the field of learning  disabilities.',\n",
       "    ' Three days later, I managed to figure out how to correct my  perceptual distortions.',\n",
       "    ' I  went  to  the  library,  picked  up  Treasure Island, and, for the first time in my life, read a book  cover to cover in just a few hours.'],\n",
       "   [' Since then, I have worked at developing techniques based  on what I discovered.',\n",
       "    ' I have had the pleasure of helping  more than  1,500 dyslexic children and adults learn to make  the  words—and the world—stand still.',\n",
       "    ' 92']],\n",
       "  'num_chunks': 2}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_text , k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_row</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>89.00</td>\n",
       "      <td>1000.72</td>\n",
       "      <td>193.88</td>\n",
       "      <td>11.14</td>\n",
       "      <td>250.18</td>\n",
       "      <td>11.54</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>75.49</td>\n",
       "      <td>440.05</td>\n",
       "      <td>81.40</td>\n",
       "      <td>5.48</td>\n",
       "      <td>110.01</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.00</td>\n",
       "      <td>770.00</td>\n",
       "      <td>147.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>192.50</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>89.00</td>\n",
       "      <td>980.00</td>\n",
       "      <td>209.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>245.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>154.00</td>\n",
       "      <td>1359.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>339.75</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>219.00</td>\n",
       "      <td>2133.00</td>\n",
       "      <td>371.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>533.25</td>\n",
       "      <td>25.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_row  \\\n",
       "count       261.00           261.00           261.00                   261.00   \n",
       "mean         89.00          1000.72           193.88                    11.14   \n",
       "std          75.49           440.05            81.40                     5.48   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%          24.00           770.00           147.00                     8.00   \n",
       "50%          89.00           980.00           209.00                    11.00   \n",
       "75%         154.00          1359.00           260.00                    15.00   \n",
       "max         219.00          2133.00           371.00                    25.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  num_chunks  \n",
       "count            261.00                     261.00      261.00  \n",
       "mean             250.18                      11.54        1.62  \n",
       "std              110.01                       5.70        0.60  \n",
       "min                0.00                       0.00        0.00  \n",
       "25%              192.50                       8.00        1.00  \n",
       "50%              245.00                      12.00        2.00  \n",
       "75%              339.75                      15.00        2.00  \n",
       "max              533.25                      25.00        3.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_text)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b13eae9ac74b2d9a2969ea36a990e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#split each chunk into each item\n",
    "pages_and_chunks =[]\n",
    "for item in tqdm(pages_and_text):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict={}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        #join rge sentences together into a paragraph like structure\n",
    "        joined_sentence_chunk = \"\".join (sentence_chunk).replace(\"  \",\" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "        \n",
    "        #get some states on our chunks\n",
    "        chunk_dict[\"chunk_char_count\"] =len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk)/4\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "len(pages_and_chunks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 59,\n",
       "  'sentence_chunk': \"Being there convinces him beyond a doubt that he is lacking in intelligence. In first grade, they only hinted about his stupidity. Now it has been confirmed. If he isn't put into a special education class, P. D. might be held back a year or even two years during elementary school. Being a year or two older than the other kids might be embarrassing in the classroom, but his size and advanced development in non-academic areas may provide him with advantages in physical education, music and art, as well as recess and after-school activities. To compensate and find some form of self-esteem, P. D. may adopt any number of interests, none of which has to do with reading and writing. It could be a sport, visual 89\",\n",
       "  'chunk_char_count': 715,\n",
       "  'chunk_word_count': 127,\n",
       "  'chunk_token_count': 178.75}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks , k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>424.00</td>\n",
       "      <td>424.00</td>\n",
       "      <td>424.00</td>\n",
       "      <td>424.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>89.58</td>\n",
       "      <td>599.29</td>\n",
       "      <td>103.00</td>\n",
       "      <td>149.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>74.07</td>\n",
       "      <td>322.91</td>\n",
       "      <td>54.79</td>\n",
       "      <td>80.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.00</td>\n",
       "      <td>321.50</td>\n",
       "      <td>57.75</td>\n",
       "      <td>80.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>90.50</td>\n",
       "      <td>612.00</td>\n",
       "      <td>109.50</td>\n",
       "      <td>153.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>152.25</td>\n",
       "      <td>839.25</td>\n",
       "      <td>146.00</td>\n",
       "      <td>209.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>219.00</td>\n",
       "      <td>1361.00</td>\n",
       "      <td>276.00</td>\n",
       "      <td>340.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count       424.00            424.00            424.00             424.00\n",
       "mean         89.58            599.29            103.00             149.82\n",
       "std          74.07            322.91             54.79              80.73\n",
       "min         -41.00              2.00              1.00               0.50\n",
       "25%          27.00            321.50             57.75              80.38\n",
       "50%          90.50            612.00            109.50             153.00\n",
       "75%         152.25            839.25            146.00             209.81\n",
       "max         219.00           1361.00            276.00             340.25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page number increases to 1843 as there would be chunks belonging to same page as well , thus unique no of pages are still the same\n",
    "\n",
    "Hmm looks like some of our chunks have quite a low token count.\n",
    "\n",
    "How about we check for samples with less than 30 tokens (about the length of a sentence) and see if they are worth keeping?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 0.75 | Text: Ill\n",
      "Chunk token count: 12.0 | Text: Draw a circle around the intersecting point. 156\n",
      "Chunk token count: 6.5 | Text: What Dyslexia Really Is 16\n",
      "Chunk token count: 0.75 | Text: 196\n",
      "Chunk token count: 20.5 | Text: that which is here or which has been mentioned. [Give me the ball. Open the book.]\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 30\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like many of these are headers and footers of different pages.\n",
    "\n",
    "They don't seem to offer too much information.\n",
    "\n",
    "Let's filter our DataFrame/list of dictionaries to only include chunks with over 30 tokens in length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'sentence_chunk': \"THE GIFT OF DYSLEXIA W h y Some of the Smartest People Can't Read and How They Can Learn Ronald D. Davis with Eldon M. Braun A Perigee Book\",\n",
       "  'chunk_char_count': 139,\n",
       "  'chunk_word_count': 29,\n",
       "  'chunk_token_count': 34.75},\n",
       " {'page_number': -40,\n",
       "  'sentence_chunk': \"Contents Foreword by Dr. Joan Smith Author's Note Preface Acknowledgments Part One What Dyslexia Really Is Chapter 1 The Underlying Talent Chapter 2 The Learning Disability Chapter 3 Effects of Disorientation Chapter 4 Dyslexia in Action Chapter 5 Compulsive Solutions Chapter 6 Problems with Reading Chapter 7 Spelling Problems Chapter 8 Math Problems Chapter 9 Handwriting Problems Chapter 10 The Newest Disability: A D D Chapter 11 Clumsiness Chapter 12 A Real Solution Part Two Little P . D . —A Developmental Theory of Dyslexia Chapter 13 How Dyslexia Happens Chapter 14 The Two-Year-Old and the Kitten vii\",\n",
       "  'chunk_char_count': 611,\n",
       "  'chunk_word_count': 98,\n",
       "  'chunk_token_count': 152.75}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 71,\n",
       "  'sentence_chunk': 'The Gift 102 When a disorientation has occurred, the brain no longer sees what the eyes are looking at, but what the person is thinking, as though the eyes were seeing it. The brain no longer hears what the ears are hearing, but what the person is thinking, as though the ears were hearing it. The body no longer feels what its senses are feeling, but what the person is thinking, and so on. One aspect of multi-dimensional thinking is the ability of the thinker to experience thoughts as realities. Reality is what the person perceives it to be, and the disorientation alters the perception. The person\\'s thoughts become the person\\'s perceptions, so the thoughts are reality to that individual. A Creative Process If \"necessity is the mother of invention,\" then multi- dimensional thinking must be its father. This concept helps us understand how Leonardo da Vinci could conceptual- ize a submarine 300 years before the invention of a device that could pump the water out of it. We see how he could envision a helicopter 400 years before there was an engine that could power one. There is little doubt Leonardo experienced flight and underwater travel hundreds of years before they became realities.',\n",
       "  'chunk_char_count': 1200,\n",
       "  'chunk_word_count': 204,\n",
       "  'chunk_token_count': 300.0}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks_over_min_token_len,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Sentences Transformers library provides an easy and open-source way to create embeddings.\n",
      "Embedding: [-2.07981411e-02  3.03164814e-02 -2.01218221e-02  6.86483532e-02\n",
      " -2.55255289e-02 -8.47687386e-03 -2.07035700e-04 -6.32377341e-02\n",
      "  2.81606354e-02 -3.33353058e-02  3.02634630e-02  5.30720949e-02\n",
      " -5.03526367e-02  2.62288190e-02  3.33314389e-02 -4.51578423e-02\n",
      "  3.63043919e-02 -1.37109228e-03 -1.20171141e-02  1.14946561e-02\n",
      "  5.04510589e-02  4.70857024e-02  2.11912952e-02  5.14607430e-02\n",
      " -2.03745961e-02 -3.58889513e-02 -6.67914515e-04 -2.94393096e-02\n",
      "  4.95859236e-02 -1.05639827e-02 -1.52013991e-02 -1.31756964e-03\n",
      "  4.48196866e-02  1.56023065e-02  8.60380283e-07 -1.21387048e-03\n",
      " -2.37978902e-02 -9.09456110e-04  7.34487409e-03 -2.53933924e-03\n",
      "  5.23370393e-02 -4.68042940e-02  1.66215282e-02  4.71578613e-02\n",
      " -4.15599458e-02  9.01952444e-04  3.60279121e-02  3.42215039e-02\n",
      "  9.68226939e-02  5.94828613e-02 -1.64984949e-02 -3.51249315e-02\n",
      "  5.92519483e-03 -7.07996951e-04 -2.41031330e-02  3.49741764e-02\n",
      " -2.94746067e-02  6.04272727e-03 -9.80646070e-03  2.83218082e-02\n",
      " -1.85376145e-02  3.63212861e-02  1.30292643e-02 -3.71232964e-02\n",
      "  5.27256802e-02 -1.19706681e-02 -7.18081892e-02  1.24431131e-02\n",
      " -6.70562219e-03  7.42154568e-02  1.16357682e-02 -1.74533576e-02\n",
      " -1.82405729e-02 -1.88930277e-02  2.82414872e-02  1.32828578e-02\n",
      " -3.51910032e-02  8.87303962e-04  5.79571947e-02  3.22093144e-02\n",
      " -3.48586286e-03  4.13768068e-02  1.44357560e-02 -3.28044780e-02\n",
      " -9.79082379e-03 -3.16493884e-02  4.23871167e-02 -4.70847525e-02\n",
      " -2.08937842e-02 -1.91250574e-02 -1.22627094e-02  1.01604946e-02\n",
      "  3.91922072e-02 -2.61895489e-02  1.09028183e-02  1.35722645e-02\n",
      " -5.79266697e-02 -3.21500190e-02 -5.75723127e-03 -2.43516210e-02\n",
      "  5.23417108e-02  5.46129933e-03 -2.30996814e-02  2.57173507e-03\n",
      " -6.63346127e-02  3.54126282e-02 -1.03907762e-02  2.25409660e-02\n",
      " -1.84574444e-02 -2.42006052e-02 -4.78365198e-02 -4.79244208e-03\n",
      " -5.34137562e-02  3.01791150e-02 -1.56130549e-02 -5.51475994e-02\n",
      " -3.91875058e-02  5.92153035e-02 -3.47646847e-02  9.68123041e-03\n",
      "  2.13416182e-02  2.30417401e-02  1.91712100e-02  2.77379248e-02\n",
      " -7.73511594e-03  1.04445554e-02 -2.67719906e-02 -2.40199026e-02\n",
      " -1.92289930e-02  3.91500071e-03 -2.54714917e-02  3.61943282e-02\n",
      "  5.12867384e-02 -8.41697305e-03 -3.13829482e-02  1.47484429e-02\n",
      "  2.13939622e-02 -3.84901129e-02  2.01945901e-02  1.20765474e-02\n",
      " -3.12088290e-03  7.84028601e-03  3.30342143e-03 -4.94357385e-02\n",
      "  5.83887100e-02  3.26128770e-03  4.84482432e-03 -4.50681299e-02\n",
      "  2.45682616e-02  3.55427451e-02 -5.32505848e-02  9.21152905e-02\n",
      "  2.04395466e-02 -3.36951837e-02 -6.19804151e-02 -2.11038850e-02\n",
      "  7.82359838e-02  5.11908159e-02  5.93170524e-02 -1.25068895e-04\n",
      "  4.96348962e-02 -1.55722499e-02 -3.35676433e-03  1.82015728e-02\n",
      " -2.73444075e-02 -1.08771687e-02  1.41476076e-02  1.09877139e-02\n",
      "  4.32551745e-03  8.23311806e-02 -9.85419611e-04  7.58791342e-02\n",
      "  9.44992434e-03  2.37687938e-02  1.61929317e-02  6.24994636e-02\n",
      "  4.75922823e-02 -3.92621243e-03  9.07523856e-02  4.49874066e-02\n",
      " -3.47131491e-02  2.14077644e-02 -3.35604399e-02  4.93850484e-02\n",
      "  1.08670024e-02  2.63447426e-02 -3.26089375e-02  8.00303370e-02\n",
      "  9.29762051e-03  7.16571510e-03 -2.79171914e-02 -3.06820869e-02\n",
      "  4.01064055e-03 -4.93906625e-02 -3.13775335e-03  4.00537252e-02\n",
      " -3.97855341e-02  5.48014604e-02  1.35884893e-05 -8.38373899e-02\n",
      " -1.21547412e-02  3.40949968e-02  3.22402944e-03  6.11846298e-02\n",
      "  5.60066402e-02  9.62875038e-03  2.54616309e-02 -4.64168973e-02\n",
      " -3.98900658e-02  7.68132731e-02  2.28408724e-02 -2.26567276e-02\n",
      " -1.91192981e-02 -6.53027669e-02  4.56781201e-02 -4.43660840e-03\n",
      "  1.49631491e-02 -2.15078592e-02  2.74239713e-03  1.90359019e-02\n",
      "  5.91888055e-02 -2.47569121e-02  3.66144963e-02  5.63083626e-02\n",
      " -8.86445399e-03 -1.74325518e-02 -1.03289378e-03  2.47667693e-02\n",
      "  1.30763371e-02  5.04633375e-02 -5.28492546e-03  5.92396855e-02\n",
      "  6.29906356e-02 -4.36783433e-02 -4.97831888e-02  5.56297228e-02\n",
      " -2.44854484e-02 -8.26755688e-02  2.04911567e-02 -1.06446326e-01\n",
      "  6.64835097e-03  2.97303461e-02 -2.36440338e-02 -8.84612836e-03\n",
      "  2.45558284e-03 -3.35234702e-02  7.52213076e-02 -5.89880086e-02\n",
      " -3.67808230e-02  3.41542661e-02  5.41130193e-02 -1.74905229e-02\n",
      "  1.33920601e-02  4.71682250e-02  1.46117033e-02 -2.12310702e-02\n",
      " -6.55339584e-02  1.23857874e-02  2.76075024e-02 -8.02160706e-03\n",
      " -4.59636450e-02 -8.22449010e-03  9.16955899e-03 -1.56398527e-02\n",
      "  7.54615199e-03  1.58317864e-03 -3.03959530e-02 -5.10671847e-02\n",
      "  1.96313243e-02  1.26263369e-02 -1.51735428e-03  2.02890877e-02\n",
      "  1.37817832e-02  1.49110621e-02  2.50766519e-02 -3.62871103e-02\n",
      "  1.08084520e-02  2.74132797e-03  1.81510337e-02  5.39872050e-02\n",
      " -4.74542044e-02 -4.28731069e-02 -2.89914850e-02  2.13234797e-02\n",
      " -3.85161676e-02  6.31922260e-02 -5.77976182e-02  3.77890165e-03\n",
      " -2.54394747e-02 -1.77195747e-04  9.08244587e-03  1.59095861e-02\n",
      "  4.11798656e-02 -3.94369215e-02 -9.64433327e-03  1.30792009e-02\n",
      "  6.87962323e-02  4.32193242e-02  7.54087872e-04  6.77741915e-02\n",
      "  4.93705943e-02 -3.47821042e-03 -1.06054442e-02  6.72494248e-03\n",
      " -1.39062526e-02  4.88276742e-02 -1.05735948e-02  3.50230793e-03\n",
      "  2.90217670e-03  2.40043998e-02  1.20272897e-02 -2.09796410e-02\n",
      " -2.39112433e-02  3.26578915e-02 -1.01319375e-03 -5.92754129e-03\n",
      " -7.40533695e-03  3.63134174e-03 -2.26698294e-02 -2.21242551e-02\n",
      "  3.86995636e-02  1.72322337e-02  3.85921597e-02 -5.04710898e-02\n",
      " -3.42144594e-02 -4.00442891e-02 -3.57910432e-02 -4.62560691e-02\n",
      "  6.70232475e-02 -4.61650081e-03 -3.29678296e-03  2.08444018e-02\n",
      " -5.14254998e-03 -5.00850417e-02  2.22504269e-02  4.66933846e-02\n",
      "  1.36208124e-02  1.77530833e-02  4.28085914e-03 -2.79332120e-02\n",
      " -1.93421226e-02 -3.87860537e-02 -3.09555698e-02 -6.64132833e-02\n",
      " -1.13433963e-02  1.64267272e-02  1.77629814e-02 -2.28230958e-03\n",
      " -3.30087878e-02 -1.36266602e-03 -2.17933860e-02 -2.67508607e-02\n",
      " -1.26376059e-02  1.61867193e-03 -4.95672822e-02  7.85445273e-02\n",
      "  4.10961695e-02  9.65923164e-03 -1.14643229e-02  1.68855919e-03\n",
      "  5.37663810e-02  2.05534813e-03 -4.11201790e-02  1.46330381e-02\n",
      " -3.75564285e-02 -3.35689522e-02  5.19258529e-03 -6.33089170e-02\n",
      "  3.32963839e-02  8.76120105e-03  1.33855548e-03 -3.95747786e-03\n",
      " -1.61677729e-02  8.26746374e-02  4.75944728e-02 -3.43055315e-02\n",
      "  2.50881836e-02 -3.50976400e-02  3.68657112e-02  4.12653014e-03\n",
      "  4.16018665e-02 -1.35181785e-01 -4.76337560e-02 -1.20025733e-02\n",
      " -3.48892175e-02  3.25454362e-02 -2.93571409e-03 -4.85051377e-03\n",
      " -1.04223825e-01  2.78610475e-02  1.41570168e-02  3.94396111e-02\n",
      " -3.88806425e-02 -1.42463772e-02 -5.19984514e-02  8.92738719e-03\n",
      " -1.99771244e-02 -2.51724850e-02 -3.41300480e-02  1.93040967e-02\n",
      " -5.20207584e-02 -6.71999529e-02 -9.46363993e-03 -1.25586963e-03\n",
      " -5.66048436e-02  2.62097958e-02  9.91584081e-03  4.38287631e-02\n",
      "  2.26640608e-03 -3.11896577e-02 -6.25468791e-02 -3.87792252e-02\n",
      " -6.83938712e-02  4.93722707e-02  5.85507974e-02 -4.08730283e-02\n",
      " -1.98638607e-02 -2.12635100e-02  4.98037487e-02 -4.51748110e-02\n",
      " -2.37141438e-02  2.32674610e-02  1.00594826e-01  9.87114105e-03\n",
      " -1.38013968e-02 -5.21041043e-02  9.08209290e-03  1.72428042e-02\n",
      "  5.91432191e-02  2.62337346e-02 -7.04637868e-03 -1.50031885e-02\n",
      " -3.76657187e-03  6.28260849e-03 -5.23982719e-02 -4.96638231e-02\n",
      "  3.06610763e-02 -3.33642215e-03  2.34911907e-02 -8.58829841e-02\n",
      " -4.62449305e-02  5.59701137e-02  3.09034251e-04  2.01728735e-02\n",
      " -2.98066647e-03  1.76645648e-02  1.54669350e-02 -7.41719306e-02\n",
      "  7.34985573e-03 -1.05013978e-02  2.45247353e-02  1.36879431e-02\n",
      " -1.17803561e-02  4.51544039e-02  3.29039395e-02 -3.50393238e-03\n",
      " -2.71315724e-02 -5.27364872e-02 -4.60164808e-02  2.22849734e-02\n",
      "  2.62272730e-02  5.56150544e-03  1.45788193e-02 -2.97144838e-02\n",
      "  3.57042663e-02  2.22534239e-02  3.89618091e-02 -7.92635158e-02\n",
      " -9.01090540e-03  2.19012778e-02 -5.49063226e-03  8.69965646e-03\n",
      "  4.33030687e-02 -2.12631766e-02  1.13292364e-02 -6.33699298e-02\n",
      "  3.63723114e-02  2.67442726e-02 -6.64251000e-02  1.70500055e-02\n",
      " -2.79692095e-02  2.36353069e-03 -1.81953404e-02  1.52954999e-02\n",
      " -8.50430317e-03  1.16648469e-02 -9.75922346e-02 -2.92093307e-02\n",
      " -5.42547107e-02  3.61234620e-02  3.25115435e-02  8.26974772e-03\n",
      " -2.96543643e-04  1.11556025e-02 -3.85188982e-02  2.36161929e-02\n",
      "  9.85917822e-03  5.73998354e-02  4.86060008e-02 -1.37579776e-02\n",
      " -6.19214587e-03  1.11972485e-02 -3.37175131e-02 -1.10515682e-02\n",
      " -7.08333403e-02 -1.01816906e-02 -3.66010815e-02 -1.55561240e-02\n",
      " -2.13109273e-02 -1.02760019e-02 -4.35734130e-02  5.55186234e-02\n",
      " -3.76547314e-02  5.29252179e-02 -3.45224552e-02 -2.43013934e-03\n",
      "  7.25553036e-02  4.45071096e-03  4.71416079e-02 -9.43879131e-03\n",
      " -1.98978111e-02  5.71899302e-02  8.60540494e-02 -5.25058173e-02\n",
      " -1.39550585e-02  1.17373643e-02  1.33974273e-02 -4.73052561e-02\n",
      " -5.41673712e-02  4.62725535e-02 -2.58970950e-02  1.51415868e-02\n",
      "  3.38944048e-02 -3.78252612e-03 -5.76043203e-02 -1.60082616e-02\n",
      "  2.42738444e-02  3.37360129e-02 -1.96821280e-02 -2.53464654e-02\n",
      " -4.75616977e-02 -5.68756089e-02 -2.28193700e-02  3.83186750e-02\n",
      " -1.78330801e-02  1.35962814e-02  7.85983982e-04  9.74011421e-03\n",
      "  3.34298648e-02 -2.60134637e-02 -7.38569582e-03  3.56451273e-02\n",
      " -2.68531796e-02 -7.53623992e-02 -2.66983788e-02 -4.46457787e-33\n",
      " -3.31645384e-02  1.41703943e-02 -3.92909087e-02 -3.46318074e-02\n",
      " -5.88679779e-03 -1.18212486e-02  1.53950844e-02  1.18474634e-02\n",
      "  1.07757989e-02  3.62141058e-02  7.87955895e-03 -2.31845006e-02\n",
      "  1.07623069e-02  1.72345266e-02  9.54219839e-04  2.83640809e-02\n",
      "  2.37420369e-02 -1.48057006e-02  1.24199281e-03  3.52354674e-03\n",
      "  2.33735628e-02  5.58308028e-02  5.38328402e-02 -3.74078490e-02\n",
      " -2.11805161e-02  1.52705659e-04 -7.27783842e-03  5.50560514e-03\n",
      "  3.05824336e-02  4.54633869e-02 -3.35787050e-02  3.16142179e-02\n",
      " -2.56392895e-03  3.96354906e-02 -1.47572421e-02  5.67167141e-02\n",
      " -5.62787689e-02 -5.04599465e-03  3.56154516e-02 -2.76199523e-02\n",
      " -2.32292209e-02 -4.63292636e-02 -3.70919965e-02 -4.23188694e-02\n",
      "  3.70306186e-02  7.88718183e-03  3.85176614e-02  1.74775335e-03\n",
      "  5.62762748e-03  6.18104311e-03 -6.90269321e-02 -9.42970160e-03\n",
      " -7.74672907e-03  1.68350488e-02  1.22766877e-02  2.26407181e-02\n",
      "  1.21009164e-02  1.11743705e-02  1.21538769e-02 -1.16862264e-02\n",
      " -4.41612266e-02  2.30048075e-02  2.20671613e-02 -5.87505065e-02\n",
      " -3.96428369e-02  6.83134943e-02 -3.29948552e-02 -3.66774388e-02\n",
      " -3.53655852e-02  1.76184960e-02  6.95644086e-03  5.92692494e-02\n",
      "  4.12155949e-02  7.98109472e-02 -5.36567578e-03  1.14238160e-02\n",
      " -2.96389144e-02 -1.15411989e-02  2.22811792e-02  7.93191139e-03\n",
      "  2.60356367e-02  1.28212059e-02  1.71346348e-02 -6.90191844e-03\n",
      " -1.07603790e-02  1.35715595e-02 -9.90764471e-04 -6.16075136e-02\n",
      "  4.40513529e-02 -8.26502044e-04 -2.78341454e-02 -1.23619512e-02\n",
      "  1.34629672e-02 -3.85745429e-02  1.08704786e-03  2.18712818e-02\n",
      " -3.32398899e-02  1.84615292e-02 -5.10105584e-03  3.74664888e-02\n",
      " -3.67547153e-03 -2.19246950e-02 -4.96482570e-03 -9.59827099e-03\n",
      "  2.33591422e-02  1.04876310e-02  4.38722149e-02 -1.51424622e-02\n",
      " -6.30309880e-02  8.23262054e-03 -1.09130945e-02 -4.06409353e-02\n",
      " -6.21690676e-02  2.21326258e-02 -2.71434449e-02  4.05540131e-02\n",
      " -8.09447560e-03 -1.76410039e-03  3.01525909e-02 -5.42267412e-03\n",
      " -4.69822250e-02 -1.73768289e-02  4.11631316e-02  3.20636220e-02\n",
      " -2.22944003e-02 -1.58162042e-02 -4.50720601e-02  5.69486357e-02\n",
      "  4.71596457e-02 -5.78059070e-02  1.32474834e-02 -4.71284427e-03\n",
      "  1.66824591e-07  4.81090210e-02  5.03628179e-02  5.45264445e-02\n",
      "  2.07568705e-02 -1.19081317e-02 -6.37492863e-03  5.26375044e-03\n",
      "  7.21949041e-02 -2.21763253e-02  2.20102966e-02 -9.90452245e-04\n",
      " -1.37162833e-02  6.89207390e-03  2.46912371e-02 -1.39462024e-01\n",
      "  2.56979442e-03 -4.64828089e-02 -4.04967666e-02 -6.08556122e-02\n",
      " -1.53212901e-02  1.36129856e-01  9.45034325e-02  4.25741449e-02\n",
      "  4.67131473e-02 -2.30677240e-02 -1.20965485e-02  3.86673585e-02\n",
      "  2.11663754e-03 -2.51472909e-02 -1.15076294e-02 -3.46506685e-02\n",
      " -2.29534339e-02 -6.33849762e-03 -3.05175465e-02 -1.56236580e-02\n",
      "  1.39513761e-02  3.27491754e-04  2.00334867e-03  4.15105652e-03\n",
      " -2.22925283e-02 -3.62589024e-02 -2.36578751e-02 -1.87817682e-02\n",
      " -1.96288731e-02  4.52125743e-02 -8.12569931e-02 -2.14568954e-02\n",
      " -4.41542752e-02 -2.68476456e-02  2.01974697e-02  2.82994588e-03\n",
      " -1.95011403e-02 -3.45331058e-02  2.26913821e-02  3.78325358e-02\n",
      " -1.02543915e-02 -2.19744141e-03 -8.96744505e-02 -4.50030342e-02\n",
      "  8.09698272e-03 -2.05805264e-02 -2.02997793e-02 -2.09922623e-02\n",
      " -1.79405082e-02  5.81897497e-02 -7.63651449e-03  1.50847109e-02\n",
      "  1.78279847e-34  4.86179255e-02  4.22228016e-02  4.71596010e-02\n",
      "  5.89047149e-02  3.99783999e-02 -5.27070761e-02  1.56905577e-02\n",
      " -5.25080424e-04  1.13652358e-02 -6.56410977e-02 -2.20849942e-02]\n",
      "\n",
      "Sentence: Sentences can be embedded one by one or as a list of strings.\n",
      "Embedding: [ 4.31718044e-02 -5.38701452e-02 -3.78044732e-02  4.27235849e-02\n",
      " -2.35409737e-02  3.44861113e-02  2.89587826e-02  1.92811759e-03\n",
      "  2.41732225e-02 -3.17011960e-02  7.32856244e-02  1.25589762e-02\n",
      "  3.64620611e-02 -2.05251705e-02  2.81973705e-02 -6.87329099e-02\n",
      "  4.22230996e-02  9.31745046e-04  3.54035385e-02  1.41787305e-02\n",
      "  7.83992652e-03  2.31179316e-02 -4.84740036e-03  1.07174329e-02\n",
      "  4.39493824e-03  5.47808874e-03 -3.80339064e-02 -3.05487402e-03\n",
      "  5.72234346e-03 -6.78214207e-02 -4.88007814e-02 -1.45031810e-02\n",
      "  6.68004388e-03 -7.17479661e-02  1.64644973e-06  1.07563799e-02\n",
      " -3.60922441e-02 -2.37057172e-02 -5.22792339e-02  3.46110165e-02\n",
      " -5.42170042e-03  1.62611399e-02  1.96564645e-02  2.25395449e-02\n",
      " -2.25996319e-03  4.06342633e-02  8.17157850e-02  2.48179529e-02\n",
      "  5.31884059e-02  7.82714933e-02 -1.91813372e-02 -1.94086935e-02\n",
      " -2.62805447e-02 -2.44083479e-02  5.49406260e-02  1.90318711e-02\n",
      "  1.60811096e-02 -2.68895421e-02 -8.24687723e-03  7.33444244e-02\n",
      "  1.00122970e-02  2.93315127e-02  3.42869153e-03 -2.13270206e-02\n",
      " -1.62442203e-03 -5.56258811e-03 -7.64879957e-02 -5.85450232e-02\n",
      " -2.82272175e-02  7.51852756e-03  7.11226314e-02  1.95452850e-03\n",
      "  5.45923132e-03  3.22309416e-03  5.12800626e-02 -3.54105420e-02\n",
      " -5.03608696e-02  4.70519289e-02  5.15476987e-03  1.52287195e-02\n",
      " -1.06680766e-02  3.16299610e-02 -9.09038354e-03 -4.01326828e-02\n",
      " -4.35235985e-02 -1.94969252e-02  1.65604874e-02 -4.71168309e-02\n",
      " -3.92091423e-02 -3.07756942e-02 -2.94167213e-02 -4.20826375e-02\n",
      "  2.27079936e-03 -2.78329551e-02  1.69421472e-02  7.74499541e-03\n",
      " -5.23741841e-02 -4.50039990e-02  3.83605696e-02 -4.90787067e-02\n",
      "  5.06618731e-02  1.01615656e-02 -1.25021935e-02 -4.64553246e-03\n",
      " -1.54539226e-02  1.58862732e-02  1.18369386e-02 -3.59232537e-02\n",
      " -7.76226446e-02  3.43359224e-02 -2.14709770e-02 -6.86098859e-02\n",
      " -5.46235740e-02  7.83901215e-02 -3.00703365e-02 -3.37549932e-02\n",
      " -4.04999107e-02  4.80515063e-02  9.53899324e-03  2.31399089e-02\n",
      " -8.16115886e-02 -6.51692227e-03  1.54212778e-02  7.04257190e-02\n",
      " -1.25068622e-02 -2.48266384e-02 -1.71329286e-02  6.13109767e-03\n",
      "  5.44413105e-02 -1.40566267e-02 -6.24518748e-03  3.65787297e-02\n",
      "  7.36230314e-02 -6.05682889e-03 -3.61630209e-02 -1.42203912e-03\n",
      "  4.43165451e-02 -3.14516388e-03  3.18768285e-02 -1.30948182e-02\n",
      " -3.69524583e-02 -4.98030707e-03  1.30016741e-03 -2.05213353e-02\n",
      "  2.06277575e-02  5.93871577e-03 -3.07161734e-03 -3.97511981e-02\n",
      "  4.29890119e-02  6.49802312e-02 -6.76022843e-02  5.41655645e-02\n",
      "  1.52571930e-03 -3.72908749e-02 -4.02427800e-02 -2.28771623e-02\n",
      "  1.31769866e-01  4.87876358e-03  1.39470724e-02  4.92436066e-02\n",
      "  2.49219723e-02 -8.76093749e-03 -5.38768200e-03 -2.65595056e-02\n",
      " -1.19766789e-02 -2.32006870e-02 -2.67433655e-02  5.66912210e-03\n",
      "  2.21722275e-02  4.67294268e-02 -5.78486547e-02  8.22120160e-02\n",
      " -3.36839515e-03  8.09646919e-02  1.41423335e-02  1.02393106e-01\n",
      " -5.76834520e-03 -1.15877325e-02  4.90584634e-02  5.87829687e-02\n",
      "  6.50030077e-02  4.74622399e-02 -2.89464239e-02 -1.76578586e-03\n",
      "  3.32560688e-02  2.91198008e-02  6.03811406e-02  3.73474380e-04\n",
      "  1.06576048e-02 -5.96284047e-02 -7.28600845e-02  2.95080487e-02\n",
      "  9.54462681e-03 -2.71541439e-02 -5.63305393e-02  9.66691179e-04\n",
      " -4.77728732e-02  4.67576608e-02  4.87261033e-03 -6.57519773e-02\n",
      " -1.42248739e-02  3.99872586e-02 -1.09798731e-02  7.68942833e-02\n",
      " -4.00003642e-02  2.96826698e-02  2.81303953e-02 -5.55424243e-02\n",
      "  6.31275401e-03  5.00450917e-02  1.89884249e-02  5.38683608e-02\n",
      " -1.95981823e-02  1.08600892e-02  1.64150242e-02  1.44135291e-02\n",
      "  1.71448551e-02  2.17624642e-02 -4.98863310e-02  1.56105887e-02\n",
      "  4.83738491e-03  1.87053736e-02 -3.18552880e-03  2.66863219e-02\n",
      "  5.55552393e-02 -4.88005430e-02 -3.02928910e-02  2.52110325e-02\n",
      "  1.07264845e-02  1.88270509e-02 -1.50688086e-02  3.43831815e-02\n",
      "  4.15124819e-02  1.37788747e-02 -5.54848015e-02  1.43847903e-02\n",
      " -5.88140227e-02 -6.01676106e-02  2.69856527e-02 -5.46129905e-02\n",
      "  8.14636145e-03 -1.17758885e-02  1.57441888e-02  1.43902993e-03\n",
      " -2.64554806e-02 -4.48877178e-02  4.39732820e-02 -1.06196196e-04\n",
      " -2.25905199e-02  3.00295819e-02  1.97440535e-02  7.44070392e-03\n",
      " -1.93789862e-02  8.09794851e-03  4.34860475e-02 -1.08539622e-04\n",
      " -3.77225988e-02  2.67195720e-02 -4.63157855e-02 -1.53398095e-03\n",
      "  8.05307273e-03 -4.30901684e-02 -2.13848483e-02  1.20185316e-02\n",
      "  8.41398258e-03  2.48266989e-03 -3.09566353e-02 -9.05277058e-02\n",
      " -4.76693697e-02  1.22606605e-02 -1.36466864e-02 -2.63655167e-02\n",
      " -7.65552092e-03  8.72374233e-03  2.65724733e-02  8.40085733e-04\n",
      " -5.55930333e-03 -9.29541420e-03  3.19337659e-02  5.94646409e-02\n",
      "  1.83205567e-02 -7.56546780e-02 -5.59389405e-02 -1.20871281e-02\n",
      " -3.16260494e-02  3.62186916e-02  7.53609836e-03 -6.15654588e-02\n",
      " -2.30459105e-02 -3.51670152e-03  1.23332078e-02 -9.67647694e-03\n",
      "  4.96861376e-02 -8.42256770e-02  1.52395796e-02 -1.82445142e-02\n",
      "  7.70462304e-02  9.28718075e-02  4.03725132e-02  1.11732587e-01\n",
      " -1.03270635e-02 -2.54558902e-02  2.13153344e-02 -1.16184435e-03\n",
      "  2.82593933e-03  5.06967008e-02 -3.13697197e-02 -8.14281777e-03\n",
      "  1.38386479e-02  4.66889851e-02  5.09671234e-02  3.77154201e-02\n",
      " -2.94988751e-02  3.60631980e-02 -2.61168275e-03  2.72210309e-04\n",
      " -6.71807751e-02 -6.54026568e-02 -3.43590975e-02  1.91068090e-02\n",
      "  4.13293801e-02 -1.10971006e-02  4.51952666e-02 -5.93564771e-02\n",
      "  1.06963571e-02 -1.82229802e-02 -5.65814152e-02  1.20387580e-02\n",
      "  4.44775000e-02  1.87049843e-02  1.63810104e-02  5.51151074e-02\n",
      " -2.23332513e-02  2.12861244e-02 -1.20339599e-02  3.26752812e-02\n",
      "  1.47003364e-02 -8.16685427e-03  1.12904646e-02 -3.00620701e-02\n",
      " -2.34345198e-02 -2.68646721e-02 -1.28718745e-03 -7.67190307e-02\n",
      "  2.22610449e-03 -5.89485187e-03  2.63103880e-02  2.07133056e-03\n",
      " -6.91152215e-02 -1.43792266e-02  2.68788934e-02 -3.51540223e-02\n",
      " -2.69612391e-02  2.54720007e-03 -6.48881346e-02  3.18728089e-02\n",
      "  1.70127023e-02 -4.54004444e-02 -1.80615988e-02 -1.61116645e-02\n",
      "  5.70773073e-02 -2.78269802e-03 -6.45586774e-02  7.86598027e-02\n",
      "  2.29076073e-02  6.81844028e-03 -9.11738910e-03 -2.27726474e-02\n",
      " -4.76526320e-02  4.88431081e-02 -2.09891386e-02 -2.43694074e-02\n",
      " -5.01213269e-03  6.70254007e-02  6.91373181e-03  2.25842930e-02\n",
      "  2.51125563e-02 -6.92507485e-03  8.59399140e-03  2.38977373e-02\n",
      "  3.29738297e-02 -1.05310544e-01  1.22094564e-02 -1.22263497e-02\n",
      " -5.73768578e-02  1.84311196e-02  2.97157578e-02 -6.09429218e-02\n",
      " -6.55256733e-02  3.55713032e-02  5.64321363e-03  3.34648113e-03\n",
      " -3.59686315e-02 -8.83420464e-03 -6.97894692e-02  6.89779818e-02\n",
      " -4.88209166e-03  2.23995298e-02 -3.16053927e-02 -7.41184223e-03\n",
      "  3.19351330e-02 -5.18788807e-02  2.11601369e-02 -5.03340587e-02\n",
      "  9.10579320e-03  2.13354342e-02  1.66838244e-02  3.49019878e-02\n",
      " -6.38500750e-02 -6.75629312e-03 -1.27405152e-02 -4.63366508e-02\n",
      " -1.14779919e-02  2.08778568e-02  2.44822986e-02  3.66473012e-03\n",
      " -2.86100898e-03  2.29388550e-02  2.13745795e-02 -3.48900743e-02\n",
      " -3.00388113e-02  4.78870571e-02  5.83370142e-02 -9.70495120e-03\n",
      "  1.38234068e-02 -3.27485576e-02 -8.11435864e-04  9.54225101e-03\n",
      "  1.20401718e-02  1.97231174e-02 -4.74879809e-04 -1.39225526e-02\n",
      " -5.21069840e-02 -1.75592341e-02 -5.41698895e-02 -1.17969504e-02\n",
      " -1.71030220e-02 -3.50195244e-02  3.38661373e-02 -6.76587224e-02\n",
      " -2.27607340e-02  1.95606481e-02  5.50250337e-02  1.22029129e-02\n",
      " -1.75167667e-03  7.22440798e-03  1.16349757e-02 -1.61908567e-02\n",
      " -3.37755382e-02  3.22626717e-02 -2.03813985e-02 -2.33858712e-02\n",
      " -1.29991733e-02 -1.66799836e-02  1.03070559e-02 -1.46030430e-02\n",
      " -7.79071152e-02 -8.25812370e-02 -3.38809900e-02  3.81114371e-02\n",
      "  7.86005519e-03  2.41455156e-02 -2.75715515e-02  1.30867753e-02\n",
      " -7.88597390e-03  1.78652015e-02  5.37323765e-02 -3.01823094e-02\n",
      "  1.69455484e-02  1.19570866e-02  3.52530682e-04  4.90209050e-02\n",
      " -8.57211184e-03  1.71267707e-03  4.83880006e-03 -4.10080217e-02\n",
      " -4.68119793e-02 -2.32556858e-03 -5.16775995e-02  3.10030542e-02\n",
      "  1.60961561e-02 -1.00803375e-02 -3.72483116e-03 -3.53388526e-02\n",
      "  2.95960978e-02  2.89097819e-02 -7.59911761e-02 -5.02981320e-02\n",
      " -2.11783499e-02  3.20462175e-02 -3.84538285e-02  2.45102532e-02\n",
      " -2.04188637e-02  6.02113316e-03 -9.81937349e-03  3.74778472e-02\n",
      "  3.40838619e-02  1.28863826e-02  5.67341968e-02 -8.09703395e-02\n",
      " -8.93602241e-03  1.33352950e-02 -2.51566060e-02  2.58414354e-03\n",
      " -6.51802048e-02  1.34400865e-02 -2.04682127e-02  6.53380342e-03\n",
      "  4.56972932e-03  1.99271683e-02 -6.07340224e-02  1.40691744e-02\n",
      " -5.75334206e-02  9.79802012e-03  3.55393030e-02 -2.45283544e-02\n",
      " -4.73312335e-03 -2.77492907e-02  2.34283060e-02 -8.76143313e-05\n",
      "  7.30440160e-03  1.42028863e-02  4.92806993e-02 -3.16540301e-02\n",
      " -1.34902503e-02  3.08487788e-02  2.80401763e-02 -4.33069132e-02\n",
      " -4.42284532e-02  3.80738862e-02  9.47262961e-05 -4.34896164e-02\n",
      "  1.43867368e-02  2.44332151e-03 -4.84072939e-02  1.08955568e-02\n",
      " -9.87493619e-03  4.59295176e-02  3.96379307e-02 -2.60116979e-02\n",
      "  2.48134118e-02 -5.37149422e-02  5.62824234e-02  8.81362054e-03\n",
      "  5.25076985e-02 -1.47370771e-02 -1.74380988e-02  3.45084444e-02\n",
      "  3.75523195e-02 -4.70167547e-02 -1.94911100e-02  3.82631756e-02\n",
      " -5.67596108e-02 -1.78615469e-03  2.33404543e-02 -5.88216602e-33\n",
      " -4.87188175e-02 -2.76265573e-02 -3.38240899e-02  2.66188476e-02\n",
      " -3.39277387e-02 -8.49186443e-03 -1.91250816e-02  3.00252605e-02\n",
      "  3.40781882e-02  5.11158146e-02 -1.92480031e-02  2.85642520e-02\n",
      "  3.66040505e-02  1.68858077e-02  4.77258675e-02  1.23802088e-02\n",
      "  2.14843657e-02  4.93653701e-04  1.21273780e-02 -5.82143962e-02\n",
      "  1.62954777e-02 -7.14258058e-03  4.80092727e-02  2.51190849e-02\n",
      "  4.60097902e-02 -2.29839869e-02 -2.05696113e-02 -3.22234351e-03\n",
      "  4.00092676e-02  3.52309458e-02 -3.43153886e-02  2.75630993e-03\n",
      " -1.25138285e-02  1.97685231e-02  5.53493435e-03  1.03744492e-01\n",
      "  5.77613153e-03 -5.65426573e-02  4.19558808e-02 -3.78830731e-02\n",
      " -3.93443145e-02 -6.24309592e-02 -2.24389695e-03 -5.46548590e-02\n",
      "  4.56133783e-02 -5.69241680e-03  3.38917002e-02 -1.44448243e-02\n",
      "  2.72104144e-03  1.11190975e-02 -5.00661880e-02 -1.61127243e-02\n",
      "  1.72818976e-03  6.88878000e-02  1.16492817e-02  2.83171441e-02\n",
      "  6.97189104e-03  2.68371757e-02 -7.72081316e-03  2.16828398e-02\n",
      "  1.15182931e-02  8.72832984e-02 -6.27269829e-03 -6.44473210e-02\n",
      " -1.58233363e-02  4.03268747e-02 -1.69728547e-02 -1.61188655e-02\n",
      " -3.75576578e-02  7.02938214e-02 -3.30486111e-02  4.66323979e-02\n",
      "  1.18028112e-02  6.51075467e-02 -1.16979126e-02 -8.28348193e-03\n",
      " -5.46905845e-02 -2.00227071e-02  8.42718757e-04 -8.19522049e-03\n",
      "  2.08357964e-02  1.37454038e-02 -1.29922677e-03 -3.94575484e-02\n",
      " -2.00185087e-02 -1.53721608e-02  1.17271692e-02 -4.40111309e-02\n",
      "  5.39267175e-02 -2.33010780e-02 -2.24211067e-02 -3.65214678e-03\n",
      "  2.92213131e-02  7.56441709e-03 -2.90923864e-02  4.01517600e-02\n",
      " -2.00853739e-02 -1.79861067e-03 -1.26236556e-02  2.51077190e-02\n",
      " -4.69286218e-02 -3.08553372e-02 -3.63375439e-04  6.01791497e-03\n",
      "  3.97508964e-02  1.38547355e-02  2.49774493e-02  1.76975224e-02\n",
      " -9.31573436e-02 -9.83684696e-03  8.44927412e-03 -1.95390582e-02\n",
      " -3.26568782e-02  5.13737090e-03  5.80932712e-03  2.08536461e-02\n",
      " -5.97834494e-03  5.86811677e-02 -1.49495509e-02 -5.72965741e-02\n",
      " -5.98234124e-03  1.95204420e-03  2.72979285e-03  6.07007090e-03\n",
      " -2.00525876e-02 -1.31687447e-02 -4.06229012e-02  5.68997897e-02\n",
      "  4.44969460e-02 -1.24308625e-02  1.96967628e-02  3.80980112e-02\n",
      "  2.30237688e-07  1.10575091e-02  4.79513071e-02  6.18298836e-02\n",
      "  4.40278985e-02  6.17667381e-03  2.58291373e-03  3.38913612e-02\n",
      " -5.32938167e-03 -2.59283744e-02 -1.26144439e-02  2.46495474e-02\n",
      " -1.68771681e-03  1.17911934e-03  2.40443107e-02 -9.77309421e-02\n",
      "  1.97368003e-02 -5.52921928e-02 -6.17424771e-02 -4.87151854e-02\n",
      "  1.11079076e-03  1.18732050e-01  8.13257992e-02  3.32449265e-02\n",
      "  4.38326932e-02 -2.49559246e-02 -3.59627269e-02  1.66319180e-02\n",
      "  5.93766849e-03 -1.43971927e-02  4.46711015e-03 -6.01986721e-02\n",
      " -5.65911569e-02 -8.21545348e-03  5.83058596e-03 -1.69482082e-02\n",
      "  9.58633889e-03  1.46733569e-02  5.05844951e-02  3.06891799e-02\n",
      "  6.60468116e-02 -2.56552286e-02 -2.78858300e-02 -3.19173336e-02\n",
      " -3.39236781e-02  1.49903325e-02 -3.03336717e-02 -6.06492022e-03\n",
      " -4.81769489e-03  1.72137320e-02 -8.23378190e-03  1.55548248e-02\n",
      "  2.69106720e-02  5.44307847e-03 -1.06898732e-02 -7.82138575e-03\n",
      " -4.44506370e-02  2.55874228e-02 -5.74760772e-02 -2.05442216e-02\n",
      " -3.07850055e-02 -1.57854892e-02 -7.07542524e-03 -4.21312265e-02\n",
      "  3.79934274e-02  6.27764612e-02 -7.67790806e-03 -3.18352990e-02\n",
      "  1.99277769e-34  1.04834503e-02 -3.39326039e-02  3.93821448e-02\n",
      "  5.53064980e-02  9.42171179e-03  1.09728212e-02 -4.91939895e-02\n",
      "  2.95023974e-02 -8.85375682e-03 -5.96248396e-02 -2.37825606e-02]\n",
      "\n",
      "Sentence: Embeddings are one of the most powerful concepts in machine learning!\n",
      "Embedding: [-2.98611429e-02 -1.37522537e-02 -4.75401767e-02  2.72126645e-02\n",
      "  3.40054147e-02  3.16465832e-02  4.26963009e-02  3.29792872e-03\n",
      "  4.35717404e-02  2.53837276e-02  3.02529484e-02  3.21131200e-02\n",
      " -3.99913192e-02  1.28760878e-02  6.70219883e-02 -7.92899877e-02\n",
      "  4.68772538e-02  2.40266304e-02 -2.07997374e-02 -1.07433060e-02\n",
      " -1.19410316e-02 -5.39290272e-02  4.21055071e-02  2.23589037e-02\n",
      " -2.98949461e-02  8.35980196e-03  1.58385374e-02 -4.80236076e-02\n",
      "  1.88435672e-03 -1.67520996e-02 -2.15628780e-02 -3.88488509e-02\n",
      "  3.06273643e-02  4.20526452e-02  1.69483428e-06 -1.86929218e-02\n",
      " -1.24558406e-02  1.32128783e-02 -4.89039496e-02  1.34746470e-02\n",
      "  2.28873547e-02  8.81780498e-03  8.64926446e-03 -2.00949311e-02\n",
      " -3.15217786e-02 -2.53433157e-02  7.57319182e-02  3.62446494e-02\n",
      "  1.25290714e-02  3.09694856e-02  4.50759660e-03 -3.50041986e-02\n",
      " -4.42552962e-04 -9.76643618e-03  6.04544990e-02  4.03472222e-02\n",
      "  1.10734608e-02  6.56205509e-03 -5.84600121e-03  3.79776349e-03\n",
      " -4.46914397e-02  1.76404957e-02  2.45915912e-02 -3.60041717e-03\n",
      "  1.02473363e-01  3.73758562e-02  6.13310421e-03 -2.24676169e-02\n",
      "  1.46482959e-02  5.00537120e-02 -2.29907744e-02  1.12925153e-02\n",
      " -3.10552903e-02 -1.49509301e-02 -2.53137341e-03  3.20944674e-02\n",
      " -4.67056334e-02 -4.85886894e-02  2.98305377e-02  6.44215718e-02\n",
      " -3.12614031e-02  3.57407331e-02  4.16527130e-02 -5.52517921e-02\n",
      " -8.74641072e-03 -2.18629669e-02 -1.12745082e-02 -2.14435980e-02\n",
      " -1.32824434e-02 -2.04866566e-02 -1.00576049e-02  3.54763307e-02\n",
      " -7.47606531e-03 -3.70188169e-02  5.77893779e-02 -2.18169093e-02\n",
      "  4.36228141e-03  2.04380769e-02  3.36815119e-02 -4.92801592e-02\n",
      "  4.82793711e-02 -1.81006000e-03 -1.05118947e-02  4.13323641e-02\n",
      " -6.79833069e-02  1.75716281e-02 -4.43412997e-02  9.90836322e-03\n",
      " -3.81810069e-02  1.10827014e-02 -5.07279076e-02 -2.17450578e-02\n",
      " -1.03836162e-02  4.60332669e-02  1.55863101e-02 -4.21366058e-02\n",
      " -2.72146501e-02  3.22818086e-02 -4.24738638e-02  2.71206982e-02\n",
      " -7.41060674e-02  4.20106985e-02  2.02437602e-02  7.31810629e-02\n",
      " -8.97690095e-03 -2.31159609e-02 -3.93559225e-02 -1.46008823e-02\n",
      " -3.30910534e-02  1.12239225e-02  2.58563133e-03 -4.36859624e-03\n",
      "  1.85855143e-02  2.69934963e-02 -1.67215634e-02  3.69569883e-02\n",
      "  4.44489829e-02 -2.21723896e-02  6.72966056e-03  1.22935446e-02\n",
      "  1.71758551e-02 -2.36471160e-03  3.72263491e-02 -2.22871136e-02\n",
      "  2.94603575e-02 -2.33691186e-02  5.38471853e-03 -3.06581482e-02\n",
      " -2.38920264e-02 -2.63613742e-02 -2.01789159e-02  1.11245625e-01\n",
      " -1.99836623e-02 -3.54029872e-02  3.84143516e-02  2.53068712e-02\n",
      "  1.99551191e-02  5.53518794e-02 -1.99332498e-02 -2.16716994e-03\n",
      "  4.91092838e-02 -4.03531305e-02 -1.16977068e-02 -5.33113480e-02\n",
      "  8.29580054e-03 -5.08251935e-02 -2.65504289e-02 -1.53242070e-02\n",
      "  5.78805758e-03  2.46570143e-03 -3.44449729e-02 -1.85131188e-03\n",
      " -3.95730212e-02 -2.71690581e-02  4.93568368e-02  8.38369504e-02\n",
      "  5.43491058e-02  8.22260752e-02  1.23894745e-02 -4.79795551e-03\n",
      "  7.77339272e-04  2.98486017e-02 -1.85584407e-02  5.98795228e-02\n",
      " -6.82796119e-03  9.78144933e-04  2.85486039e-02 -7.64620211e-03\n",
      " -1.86619591e-02 -2.69287620e-02 -2.90333107e-02 -1.37871448e-02\n",
      " -2.57602637e-03 -2.20172927e-02 -1.70821436e-02 -3.81843075e-02\n",
      "  2.21505631e-02 -3.59234288e-02 -1.19439717e-02 -3.18308212e-02\n",
      " -4.80801612e-02  9.77631845e-03 -1.04868237e-03  4.15371545e-02\n",
      " -1.10973818e-02 -4.72829901e-02  1.90984290e-02 -5.31177372e-02\n",
      "  2.11326014e-02 -2.53072148e-03  5.61055318e-02 -1.33794891e-02\n",
      " -5.95860090e-03 -1.20308697e-02  4.63929996e-02 -2.81909369e-02\n",
      "  2.25355141e-02 -2.50473293e-03 -3.52454148e-02  2.55494937e-02\n",
      "  9.10391938e-03  3.25213443e-03  2.55967001e-03 -1.25624258e-02\n",
      " -3.51496264e-02 -4.28946391e-02 -2.32327497e-03  2.41020638e-02\n",
      " -5.16838534e-03  1.68739706e-02  5.52647281e-03  2.36792732e-02\n",
      "  5.65164052e-02 -3.47868651e-02 -6.34516999e-02 -7.45629007e-03\n",
      " -1.78447496e-02  5.35898954e-02  2.67291237e-02 -8.74199197e-02\n",
      "  1.04196128e-02 -4.13906324e-04 -3.04449163e-03  9.14252549e-03\n",
      "  2.91529410e-02 -5.81832156e-02  6.83463141e-02 -4.08617817e-02\n",
      " -9.09786299e-03 -3.40769999e-02  3.52410898e-02 -1.02627808e-02\n",
      " -5.72466932e-04 -2.73456494e-03  1.59635898e-02  4.49068146e-03\n",
      " -2.09051464e-02  3.02770175e-02  2.46119257e-02 -1.44067686e-02\n",
      "  1.73269324e-02  1.99034112e-03  4.23051231e-02 -2.39176750e-02\n",
      " -3.25547606e-02 -1.45939561e-02  3.95101383e-02 -6.04649521e-02\n",
      " -3.02065350e-02  1.67189147e-02 -2.26817951e-02 -2.61954851e-02\n",
      " -5.51320165e-02  1.44908493e-02 -1.99246947e-02  3.99755174e-03\n",
      "  3.12609859e-02 -4.90727499e-02 -9.49700538e-04  5.39497100e-02\n",
      " -9.10086744e-03 -2.69486308e-02 -3.63159291e-02 -1.38436863e-02\n",
      " -4.45621945e-02  5.49359247e-02  2.17599492e-03  2.23445194e-03\n",
      " -5.23023028e-03 -1.47893922e-02  3.60591896e-02  1.45263216e-02\n",
      "  8.39191116e-03 -6.10361174e-02 -7.89947901e-03 -2.98303878e-03\n",
      "  3.56561481e-03  8.33992288e-02 -2.61215102e-02  8.06721747e-02\n",
      "  3.63054452e-03  1.69974286e-02  2.58604418e-02  1.09439483e-03\n",
      " -4.57063243e-02  5.55678867e-02  2.00643167e-02  4.76660877e-02\n",
      " -4.91053388e-02 -1.86080597e-02  3.34405489e-02 -2.57310756e-02\n",
      " -3.16369603e-03  7.21444264e-02 -1.61518864e-02 -1.33933611e-02\n",
      " -6.06294125e-02 -2.82187033e-02 -8.91921017e-03 -2.71165022e-03\n",
      "  8.04914441e-03 -4.95209955e-02  7.89434016e-02  2.76428275e-02\n",
      " -5.42577589e-03 -3.06724105e-03 -4.11826856e-02  1.39172617e-02\n",
      "  3.04253288e-02  1.02856122e-02  1.06679350e-02 -5.56554385e-02\n",
      " -1.75083261e-02  2.03868989e-02  8.43306817e-03  3.82471010e-02\n",
      " -3.89100350e-02 -1.61303300e-02  3.18059623e-02 -7.32970387e-02\n",
      " -1.76502001e-02 -4.79874015e-02 -5.55042289e-02 -5.00749331e-03\n",
      "  4.46753867e-04  3.57333682e-02 -8.24616349e-04 -3.34324650e-02\n",
      " -3.32417265e-02 -2.46460307e-02  2.15332285e-02  3.90858529e-03\n",
      "  2.53471583e-02  6.02989784e-03 -7.81613588e-03  1.23765375e-02\n",
      " -1.71039086e-02  2.68103648e-02  2.83682486e-03 -1.27643002e-02\n",
      "  1.00510925e-01  1.03581334e-02 -3.55143063e-02  1.56615861e-02\n",
      " -9.85950008e-02  4.58441339e-02 -3.15230079e-02 -2.35781837e-02\n",
      " -2.78350189e-02 -7.75709414e-05 -2.82363538e-02 -1.92918237e-02\n",
      "  1.87389217e-02  5.71941249e-02  2.56912597e-02 -3.20030265e-02\n",
      "  1.99074019e-02 -3.15819271e-02 -4.02062610e-02  5.77630661e-02\n",
      "  1.72974244e-02 -5.37012815e-02 -1.25325695e-02 -1.45483706e-02\n",
      " -5.76174185e-02  1.09727718e-02 -2.04728413e-02  2.85540372e-02\n",
      " -5.04399948e-02  4.36991118e-02  1.75710749e-02 -1.02343559e-02\n",
      " -9.69772339e-02 -2.99996510e-02 -2.86679436e-02  2.24936716e-02\n",
      " -1.68121196e-02 -1.43673699e-02 -8.79587419e-03 -1.69044193e-02\n",
      "  2.41557620e-02 -6.53192475e-02 -4.10800427e-02 -2.34056897e-02\n",
      " -6.76065013e-02 -1.55690545e-02  3.62358242e-02  7.83160180e-02\n",
      " -4.97516617e-02 -7.08547384e-02 -5.01179770e-02 -8.56427476e-04\n",
      "  5.44901425e-03  4.34703566e-03  9.88052711e-02 -2.16415748e-02\n",
      " -1.87751073e-02  1.15069542e-02  2.63996236e-02  1.65235605e-02\n",
      " -2.24057324e-02 -4.31826524e-02  1.31803870e-01 -2.97034718e-02\n",
      "  2.65935045e-02 -1.38888787e-02 -1.67003646e-02  3.44145149e-02\n",
      " -8.94355774e-03  6.16001301e-02 -3.42303216e-02  2.46424484e-03\n",
      " -8.14093091e-03  5.80325164e-02  5.24208471e-02 -1.53281800e-02\n",
      "  4.01382744e-02  1.51406806e-02 -3.01467348e-03 -4.97021303e-02\n",
      " -4.24307492e-03  5.77288531e-02  3.17873657e-02  4.74008396e-02\n",
      "  2.95218434e-02 -1.50122140e-02 -2.47944519e-02 -7.11501613e-02\n",
      "  2.06847731e-02  3.11488397e-02 -5.86067839e-03  1.62786338e-02\n",
      " -3.93683277e-02  5.46506532e-02  3.26595381e-02 -1.87021475e-02\n",
      " -9.79863033e-02  4.33768332e-03 -5.58157004e-02 -1.34621151e-02\n",
      "  2.88454201e-02  1.58748366e-02 -3.32564153e-02  1.44413277e-03\n",
      " -5.51108271e-02  8.24673474e-02  2.38845646e-02 -2.04838514e-02\n",
      " -4.78585716e-03  3.78722586e-02 -4.87563275e-02  3.44647206e-02\n",
      "  1.10358587e-02  1.12449918e-02  1.33263487e-02 -3.46375667e-02\n",
      " -6.92220479e-02  7.30115082e-03 -6.57011662e-03  1.73204392e-02\n",
      "  5.23061305e-03  4.48132567e-02  3.89853343e-02 -1.99275240e-02\n",
      " -1.80920698e-02  3.25937755e-02 -2.02027038e-02  4.86247707e-04\n",
      " -8.88761971e-03 -1.91347916e-02  2.50686184e-02  4.74019162e-02\n",
      "  2.18657590e-03 -1.69987939e-02  3.62670794e-02  3.46247293e-03\n",
      "  4.21927823e-03  8.04170966e-02  3.10627595e-02 -1.04945642e-03\n",
      " -3.55466381e-02  4.34837341e-02 -3.06218304e-02 -3.03192325e-02\n",
      " -4.13175821e-02 -1.05258515e-02 -2.35242043e-02 -1.86771657e-02\n",
      "  4.42931056e-03  5.45056164e-02 -6.05017692e-02  2.48421766e-02\n",
      " -3.36967669e-02 -4.54169251e-02 -2.63173170e-02  6.98054815e-03\n",
      "  6.92871213e-02 -2.04491410e-02 -1.96813233e-02 -9.72561352e-03\n",
      " -1.21564409e-02  7.89339654e-03  1.84750592e-03 -6.93650767e-02\n",
      "  2.43358072e-02  4.00609560e-02  3.44013236e-02 -2.84281634e-02\n",
      " -1.09431557e-02  1.38742710e-02 -4.40590875e-03  1.19350327e-03\n",
      " -8.81165564e-02  1.15931015e-02 -2.56350450e-02  5.57525456e-02\n",
      "  1.26946136e-01  5.39565906e-02 -1.41436812e-02  1.27196591e-02\n",
      " -1.32235838e-02 -5.94484322e-02  2.86704246e-02  2.57285107e-02\n",
      " -8.33769329e-03  8.17378284e-04  5.93053782e-03  3.29111032e-02\n",
      "  4.12751958e-02 -5.77961886e-03 -1.71124525e-02  1.06227333e-02\n",
      " -2.19601709e-02 -4.97207008e-02  2.53765807e-02 -5.60257726e-33\n",
      " -1.35397157e-02 -3.77958529e-02 -2.67922878e-03 -3.69652407e-04\n",
      " -1.98267289e-02  5.47051383e-03  6.02688035e-03  1.93068795e-02\n",
      "  3.87973548e-03  2.97698714e-02 -1.88229028e-02  2.20038509e-03\n",
      "  8.17020051e-03  1.61965284e-02  3.17529365e-02 -6.83412887e-03\n",
      "  2.19252463e-02  4.37701674e-04  2.96859182e-02 -2.62557399e-02\n",
      "  6.49384549e-03  3.56025398e-02  1.58608588e-03 -4.76583913e-02\n",
      " -5.26238531e-02  3.78274061e-02  3.54342535e-02 -3.10347639e-02\n",
      "  7.96404947e-03  5.48469387e-02 -3.56443599e-02  9.10136476e-03\n",
      " -9.45984758e-03 -4.63287458e-02 -1.63907204e-02  6.32452890e-02\n",
      " -1.38588538e-02 -5.95724955e-02 -1.57990456e-02  2.01887153e-02\n",
      " -1.98292285e-02 -3.49211581e-02  2.27937698e-02 -5.91621846e-02\n",
      "  4.18854617e-02  1.20738824e-03  5.19158579e-02 -1.88436266e-02\n",
      " -3.12102251e-02  2.34932136e-02 -7.41029158e-02 -2.76590377e-04\n",
      " -1.51720140e-02  6.11713491e-02  1.25065118e-01 -1.28459064e-02\n",
      " -1.12671554e-02  1.51760050e-03 -8.09153542e-02  1.12689575e-02\n",
      " -1.97573584e-02  2.74268258e-02  9.40998830e-03 -9.58755333e-03\n",
      "  2.54850443e-02  6.81659132e-02 -1.83453467e-02 -1.00963980e-01\n",
      " -9.45247896e-03 -5.27007319e-03  1.98683981e-02  9.80848372e-02\n",
      "  3.15633267e-02  5.30422628e-02  3.75123583e-02 -6.64209202e-02\n",
      " -5.92879988e-02 -1.57074351e-02  1.76609531e-02 -5.81073500e-02\n",
      "  2.23230720e-02  1.29869329e-02 -3.30261067e-02  9.96951479e-04\n",
      " -9.87092033e-03 -3.12955305e-02  2.28525023e-03 -4.91250493e-02\n",
      "  1.47694228e-02 -1.83367599e-02 -4.16306518e-02  3.76896933e-02\n",
      "  3.35410275e-02 -7.97111690e-02  4.01299670e-02  1.59071963e-02\n",
      "  5.06086834e-03  4.28808369e-02  2.29760278e-02 -4.13350835e-02\n",
      " -3.10502984e-02 -5.26404381e-02 -4.95404527e-02 -2.94256806e-02\n",
      "  5.94924614e-02 -2.59802733e-02  3.02497204e-02  8.80402979e-03\n",
      " -4.84466851e-02 -2.00851150e-02  9.82183218e-03 -7.89193660e-02\n",
      "  4.52871807e-03 -9.34896711e-03  9.23311524e-03 -3.17361876e-02\n",
      "  2.10833177e-02  6.37125690e-03  3.36347893e-02  3.83613631e-02\n",
      " -4.55527864e-02  1.08126178e-03 -9.83322412e-03  7.70194782e-03\n",
      " -2.87617911e-02 -1.74959823e-02 -4.27817088e-03  2.81287134e-02\n",
      "  4.97339591e-02 -7.45570213e-02 -1.07008992e-02 -7.66044995e-03\n",
      "  2.33968919e-07  1.52483461e-02  8.39613825e-02  3.67242806e-02\n",
      " -3.69249061e-02  3.64752077e-02  4.26422022e-02 -4.39831475e-03\n",
      "  1.78133808e-02 -2.67076232e-02 -7.13904342e-03  5.59975281e-02\n",
      "  3.13966535e-02  2.13436899e-03  3.90371345e-02 -8.78527984e-02\n",
      " -2.21659876e-02 -2.47735288e-02 -1.18189622e-02 -7.89705478e-03\n",
      " -2.08857823e-02  4.30555977e-02  1.07643597e-01  4.40617949e-02\n",
      "  1.47962719e-02  2.44862698e-02 -3.86268310e-02  1.80743076e-02\n",
      " -1.47839938e-03  7.74166733e-02 -4.19565775e-02 -3.80529352e-02\n",
      "  3.61256711e-02  1.59032585e-03  1.95324030e-02 -2.00080462e-02\n",
      "  4.22537960e-02  3.06110885e-02 -3.53686465e-03  5.93100162e-03\n",
      " -2.23223735e-02 -2.07131393e-02 -3.62908887e-03  1.74653586e-02\n",
      " -4.08758968e-02  5.91595173e-02 -5.89099452e-02 -3.96753959e-02\n",
      " -3.33529301e-02  1.02161057e-02  6.97292387e-03  7.70389736e-02\n",
      " -1.86911505e-02 -1.82568263e-02 -2.42319200e-02 -3.40699288e-03\n",
      " -3.60554680e-02  4.33389135e-02 -3.48602571e-02  5.27769066e-02\n",
      "  2.89709363e-02 -4.98462431e-02 -1.94749199e-02  1.16398521e-02\n",
      " -3.04615460e-02  8.04637894e-02  6.56248480e-02 -2.84533482e-02\n",
      "  1.81615301e-34 -4.19158954e-03 -2.57882662e-02  5.17320037e-02\n",
      "  4.94420417e-02  1.32475980e-02 -4.21994887e-02 -1.12458598e-02\n",
      " -2.61519253e-02  5.51130474e-02  2.20024381e-02 -2.51170434e-02]\n",
      "\n",
      "Sentence: Learn to use embeddings well and you'll be well on your way to being an AI engineer.\n",
      "Embedding: [-2.20730547e-02  2.08950844e-02 -6.03005029e-02  8.43949430e-03\n",
      "  4.37650867e-02  1.55070238e-02  4.99907956e-02 -3.03232670e-02\n",
      "  4.94783968e-02  2.35512573e-02  3.29350270e-02  1.53878108e-02\n",
      " -6.68355003e-02  1.11002855e-01  6.92677274e-02 -2.31888033e-02\n",
      "  3.79102528e-02 -4.94144717e-03 -1.57800969e-02 -3.45476530e-02\n",
      " -2.65053045e-02 -2.47879811e-02 -1.86141673e-02  3.00361905e-02\n",
      " -2.81185675e-02 -8.75127688e-03 -3.30773811e-03 -2.06115581e-02\n",
      "  1.03315748e-02 -1.51483342e-02 -3.48331034e-02 -2.63247769e-02\n",
      "  2.06907913e-02  3.79108898e-02  1.81912947e-06 -2.44284840e-03\n",
      " -1.80560315e-03  5.61761856e-03 -2.79870015e-02  1.54703567e-02\n",
      "  3.06456313e-02  3.72600704e-02 -1.55611672e-02  2.54414622e-02\n",
      " -6.42072633e-02  3.16353478e-02  6.63442239e-02  3.80970016e-02\n",
      "  5.57844639e-02  5.31659126e-02 -9.69289336e-03 -3.61423716e-02\n",
      "  3.72434594e-02 -4.67832480e-03  5.14575429e-02  1.00058615e-02\n",
      "  4.90287133e-03  1.41562326e-02  4.95100170e-02  3.32953339e-03\n",
      " -3.21100950e-02  4.42387983e-02  3.27416398e-02 -7.90620316e-03\n",
      "  1.07809782e-01  7.32945353e-02  3.36702913e-02 -4.28345427e-02\n",
      "  1.05966423e-02  2.05654018e-02 -2.02669837e-02  1.04964050e-02\n",
      " -1.97615996e-02 -2.89532873e-05 -2.61862315e-02 -1.85173769e-02\n",
      " -3.44269760e-02 -4.08621915e-02  2.32571419e-02  2.14196015e-02\n",
      "  1.31320357e-02 -3.27211805e-02 -1.91425551e-02 -2.86572408e-02\n",
      " -1.16858557e-02  1.21910665e-02  1.05248066e-02 -3.39584760e-02\n",
      "  3.08902003e-03 -4.44888175e-02  2.65105441e-02  1.09535968e-02\n",
      "  2.51450632e-02 -6.48842938e-03  4.54374589e-03 -2.02785134e-02\n",
      " -1.03216842e-02  2.06590313e-02 -1.65313799e-02 -2.45611388e-02\n",
      "  5.47549278e-02  2.68261321e-02  2.95511149e-02  3.86754572e-02\n",
      " -7.76720569e-02  3.80055495e-02 -2.98364796e-02  7.96886310e-02\n",
      " -3.00943162e-02  7.57849030e-03 -6.89827055e-02 -2.92666890e-02\n",
      " -2.35579181e-02  3.48197930e-02  2.52938457e-02 -4.53817844e-02\n",
      " -1.57938469e-02  4.39030901e-02 -4.04335596e-02  8.32536537e-03\n",
      " -2.84665748e-02  4.94934358e-02  2.41276305e-02  3.02191880e-02\n",
      " -4.99590300e-02 -5.94533272e-02 -3.70175615e-02  1.30331218e-02\n",
      " -3.36468481e-02  3.45589742e-02 -1.44523820e-02  2.57639643e-02\n",
      "  4.61184932e-03  2.21552104e-02 -4.93463036e-03  9.66005251e-02\n",
      " -2.72455276e-03  5.65320021e-04 -3.24247591e-02  1.31681748e-02\n",
      "  4.41607460e-02 -7.03044422e-03  6.84261248e-02 -2.28166077e-02\n",
      " -2.81032384e-03 -4.23882864e-02 -1.33632226e-02 -5.96738532e-02\n",
      " -6.96122879e-03 -2.31901519e-02 -3.78851742e-02  9.80186835e-02\n",
      " -2.21729353e-02 -2.30062436e-02  3.22815143e-02  8.21806025e-03\n",
      " -7.04126433e-03  4.84078974e-02  4.23291102e-02 -2.59717461e-03\n",
      "  1.20455879e-04  1.67414248e-02  2.91197933e-02 -1.28740892e-02\n",
      " -2.41077635e-02 -3.29319239e-02 -3.50289093e-03 -3.19322348e-02\n",
      " -2.64171809e-02  2.30404865e-02  1.11636873e-02 -9.96009633e-03\n",
      " -1.75901093e-02 -2.00277683e-03  1.21595114e-02  4.67822999e-02\n",
      "  5.20881861e-02  7.21171945e-02  1.94979012e-02  1.15071982e-02\n",
      "  5.94169926e-03 -1.47833107e-02 -2.87724398e-02  6.72666058e-02\n",
      " -1.68758929e-02  5.25872782e-03 -3.39737087e-02  5.24596535e-02\n",
      " -2.59793364e-02 -4.41379920e-02  1.47454231e-03 -1.06599517e-02\n",
      " -1.51859373e-02 -1.55867159e-03  1.81505606e-02 -4.85411175e-02\n",
      "  3.67919146e-03 -6.59312904e-02 -1.49418106e-02 -3.23528759e-02\n",
      " -2.79949866e-02  1.71856824e-02 -7.87094701e-03  4.65692058e-02\n",
      "  1.47123709e-02 -7.40438998e-02 -6.52104393e-02 -5.22735305e-02\n",
      " -1.82342846e-02  5.20858839e-02  3.06305103e-02 -2.36037038e-02\n",
      "  2.42385007e-02 -1.83938723e-02 -4.83019883e-03 -2.13387031e-02\n",
      "  1.56583618e-02  9.87338927e-03 -4.25561033e-02  6.00465015e-03\n",
      " -3.14498297e-03  4.51508630e-03 -1.52780465e-03  1.13731902e-02\n",
      " -6.96354657e-02 -3.37257199e-02  1.33407079e-02  4.87288414e-03\n",
      " -7.81492796e-03  4.78049517e-02 -1.59711782e-02  3.14606167e-02\n",
      "  5.15920669e-02 -4.05123010e-02 -5.06460704e-02  9.99929011e-03\n",
      " -2.00729556e-02  4.21552733e-02  3.03181689e-02 -1.00431271e-01\n",
      " -4.12019603e-02  3.43990028e-02  3.29208709e-02  1.07413437e-03\n",
      "  3.70961502e-02 -6.94237277e-02  6.52394816e-02  8.31672177e-03\n",
      "  1.68036688e-02 -2.60705557e-02  8.14490765e-03 -1.48009723e-02\n",
      " -2.65671667e-02  3.29321437e-02 -7.37515697e-03  7.23974593e-03\n",
      " -2.69329119e-02  1.71754230e-02 -2.28083115e-02 -4.75346344e-03\n",
      "  2.88569089e-02  1.30798158e-04  5.44128232e-02 -1.43379075e-02\n",
      "  1.89891476e-02 -1.32732010e-02  4.01177369e-02 -7.29275495e-02\n",
      " -2.41211243e-02  3.16216908e-02 -1.68015286e-02  8.47542100e-03\n",
      " -5.23940548e-02 -1.43882846e-02 -1.46156624e-02  6.39906898e-03\n",
      "  2.15113945e-02 -5.18960617e-02 -4.30576317e-02  2.34076027e-02\n",
      "  2.30270461e-03 -2.48434003e-02 -4.38243411e-02 -2.16570962e-02\n",
      " -6.91594929e-02  1.76770128e-02  2.12068260e-02 -2.20293142e-02\n",
      " -1.06773861e-02  9.57431272e-03  2.21988391e-02  5.51470704e-02\n",
      "  1.03682717e-02 -8.14825669e-02 -7.94703420e-03 -1.85866095e-02\n",
      "  1.20494105e-02  7.51404017e-02 -1.41215846e-02  8.83924291e-02\n",
      "  3.12628597e-02  8.12266581e-03 -2.29444932e-02  3.96010280e-02\n",
      " -2.00167857e-02  9.16027352e-02 -2.06839088e-02  5.84990792e-02\n",
      " -4.32367995e-02 -4.74755187e-03 -9.51895211e-03  5.42950118e-03\n",
      "  1.19139477e-04  6.15377724e-02 -1.68787211e-03 -4.66321334e-02\n",
      " -2.01405082e-02  1.32406447e-02  9.70687438e-03  2.73846928e-02\n",
      "  3.06639578e-02  6.71574147e-03  8.71220902e-02 -1.97374448e-03\n",
      "  8.18298385e-03  5.44372713e-03 -5.76205328e-02  1.34821851e-02\n",
      "  5.06276172e-03 -2.10568551e-02  1.25937117e-02 -5.49773499e-03\n",
      " -1.44645311e-02 -2.92567462e-02  5.53299226e-02 -2.60099489e-02\n",
      " -2.82455026e-03 -2.30902936e-02  8.89708661e-03 -2.61565186e-02\n",
      "  9.08598828e-04 -6.16204366e-02 -7.56419078e-02 -1.05932429e-02\n",
      " -1.19564068e-02  6.71458617e-02 -1.96235310e-02 -5.00934124e-02\n",
      " -3.91229168e-02 -3.07008997e-02  7.18906745e-02  9.29247681e-03\n",
      " -6.34389650e-03  7.86940043e-04 -1.36484122e-02  2.87188403e-02\n",
      "  4.01224419e-02  1.28037157e-02  1.77381057e-02 -4.75975731e-03\n",
      "  5.47173359e-02  1.10809226e-03 -2.25790311e-02 -2.80295988e-03\n",
      " -1.13696113e-01  2.55904663e-02  4.00385674e-04 -4.39810120e-02\n",
      "  1.36318589e-02 -1.54137518e-02 -4.99015227e-02 -2.32889764e-02\n",
      " -1.62987504e-03  3.95834967e-02  1.89040545e-02 -3.02702505e-02\n",
      "  2.71437895e-02  1.05689804e-03 -4.21067737e-02  3.71961072e-02\n",
      "  3.54257561e-02 -6.98274076e-02 -2.20937897e-02 -4.01495099e-02\n",
      " -1.90164018e-02 -2.69834902e-02 -1.51212616e-02  3.33365425e-02\n",
      " -9.74889845e-02  1.73102058e-02  6.21015253e-03 -2.59420183e-03\n",
      " -1.10152967e-01 -6.10185787e-02 -1.36549594e-02 -1.35034602e-02\n",
      " -6.72575012e-02 -4.05119732e-03 -6.64985552e-03  3.86573584e-03\n",
      "  9.43472143e-03 -3.86637673e-02 -1.93593837e-02  1.34934094e-02\n",
      " -4.58106697e-02  6.06737360e-02  6.06380440e-02  4.85596061e-02\n",
      " -4.56088483e-02 -5.71936667e-02 -1.55094927e-02  3.40963453e-02\n",
      "  9.48140048e-04 -9.94352531e-03  2.84658410e-02 -3.29024270e-02\n",
      " -2.83211395e-02  3.19907889e-02  2.61299480e-02 -2.74054632e-02\n",
      " -1.36353113e-02  7.47714937e-03  1.19430237e-01 -4.45807353e-02\n",
      "  1.07672736e-02 -8.69423896e-02 -2.19551641e-02  1.83874592e-02\n",
      " -1.06521631e-02 -1.89242903e-02 -3.06514278e-02 -3.04702204e-02\n",
      " -3.22214775e-02  4.12150435e-02  8.95760860e-03 -2.73179375e-02\n",
      "  9.39998124e-03 -9.57122771e-04 -1.94010567e-02 -4.92622554e-02\n",
      " -9.18891001e-03  4.66894135e-02  5.41892983e-02  2.21609324e-02\n",
      " -2.86351684e-02  5.20295165e-02  2.47590300e-02 -7.14267939e-02\n",
      " -1.26207424e-02  7.35521503e-03  2.13784985e-02  2.93513797e-02\n",
      " -2.57650558e-02  5.20562232e-02 -2.74892189e-02 -3.10242344e-02\n",
      " -9.02881101e-02  6.10371828e-02 -5.22610284e-02  2.13110968e-02\n",
      "  4.41735908e-02  3.23370695e-02  1.75648164e-02 -2.39519626e-02\n",
      " -2.69709174e-02  5.11278994e-02  2.69064456e-02 -4.51932475e-02\n",
      "  2.52652774e-03  2.44934354e-02 -2.89540663e-02  2.79992726e-02\n",
      " -1.36022484e-02 -4.32368144e-02  1.85829978e-02  7.63842181e-05\n",
      "  2.43503624e-03 -3.73323658e-03 -1.72280092e-02  1.01292403e-02\n",
      "  1.98436920e-02 -2.60017663e-02 -3.40174953e-03  1.09126391e-02\n",
      " -4.16363142e-02  3.37032080e-02 -2.81634722e-02  1.79126319e-02\n",
      " -4.53095250e-02 -1.09818606e-02 -2.20831297e-03  1.99102275e-02\n",
      "  3.56372111e-02 -3.11799180e-02  3.78752798e-02 -1.41408388e-02\n",
      " -2.16907784e-02  2.73019373e-02  3.69812781e-03  6.35386705e-02\n",
      "  1.22669684e-02 -6.02268334e-03 -7.60754058e-03 -1.86565686e-02\n",
      " -5.64713823e-03 -2.20050965e-03 -1.31825330e-02  1.67724062e-02\n",
      " -3.77264656e-02  2.97896210e-02 -5.01569249e-02  4.89088707e-02\n",
      " -6.07444905e-02 -8.39419216e-02 -5.09001091e-02  1.81768481e-02\n",
      "  6.66732267e-02 -3.30037740e-03 -2.82404572e-03 -5.35405874e-02\n",
      "  3.90340909e-02  2.19851807e-02  3.13554928e-02 -3.36527266e-02\n",
      "  1.96914244e-02  1.67883802e-02  5.04002832e-02  3.08061205e-03\n",
      " -7.24796322e-04  4.42907512e-02 -4.12958069e-03  4.29329164e-02\n",
      " -6.62552863e-02  1.16061699e-03 -2.81716529e-02  1.56886168e-02\n",
      "  9.78134051e-02  5.53594232e-02 -1.39379054e-02  2.12306883e-02\n",
      " -1.30955437e-02 -6.82027712e-02 -8.09108431e-04  4.99291308e-02\n",
      " -2.69266162e-02 -2.97804344e-02  3.84461805e-02  1.97354835e-02\n",
      "  3.37088332e-02  1.65874064e-02  5.77317178e-03 -3.04898228e-02\n",
      " -1.52511718e-02 -3.56159210e-02 -8.69221054e-03 -5.42296980e-33\n",
      "  3.24372924e-03 -3.46329734e-02  3.58932316e-02  1.83770694e-02\n",
      " -2.17505284e-02 -3.26411948e-02  2.88397912e-03  1.50463376e-02\n",
      " -1.75269553e-03 -1.99418552e-02 -6.10355567e-03  2.23846976e-02\n",
      " -8.78949184e-04  2.48684827e-02  3.39737199e-02  2.75592934e-02\n",
      "  3.37792598e-02  3.98564860e-02  2.55544912e-02  1.83042847e-02\n",
      " -2.92878747e-02  5.18079428e-03  8.37678439e-04 -3.66559774e-02\n",
      " -3.46733369e-02  3.82687338e-02  5.50825521e-03 -4.35188338e-02\n",
      "  2.44077351e-02  3.54167409e-02 -2.13442165e-02  2.86623873e-02\n",
      " -2.65391544e-04  3.73409726e-02 -8.68167635e-03  3.04786721e-03\n",
      " -2.71681882e-02 -3.85087952e-02 -6.12389669e-02 -2.00847792e-03\n",
      " -1.22079821e-02 -8.67197663e-02  3.75359086e-03 -1.77707579e-02\n",
      "  8.32476281e-03 -1.69168171e-02  7.02404007e-02  3.32233869e-02\n",
      "  4.34312895e-02  1.47017008e-02 -1.25546902e-01  1.50866490e-02\n",
      " -5.43164201e-02 -1.79156906e-03  4.99600805e-02 -1.53785814e-02\n",
      "  3.32683735e-02 -3.07708848e-02 -1.83896646e-02  9.45796352e-03\n",
      " -4.60290723e-02 -2.03866931e-03  2.62428690e-02 -5.00789322e-02\n",
      "  2.01835893e-02  6.08982742e-02 -2.01180596e-02 -2.60054879e-02\n",
      "  1.05925500e-02 -3.31153758e-02  1.62595753e-02  7.77862072e-02\n",
      " -1.90726097e-03 -5.62890386e-03  1.43715953e-02 -4.06833626e-02\n",
      " -5.14971018e-02  1.66213111e-04 -3.33068590e-03  1.44689912e-02\n",
      "  4.24915081e-04  3.04453131e-02 -1.83636677e-02  1.51183724e-03\n",
      "  2.99861878e-02 -3.68001647e-02  8.35629646e-03 -3.31025012e-02\n",
      "  2.66912226e-02  5.47832018e-03 -1.80525109e-02  2.42576785e-02\n",
      "  5.72706806e-03 -5.93372174e-02  1.04358509e-01 -9.87922680e-03\n",
      " -1.36106033e-02  5.79998642e-02  2.50108372e-02  2.89336909e-02\n",
      " -3.20522487e-02 -3.40232886e-02 -3.41698788e-02 -2.76980978e-02\n",
      "  6.47003055e-02  1.50797674e-02 -1.61925405e-02  3.03265732e-02\n",
      " -2.67188158e-02 -3.67773920e-02 -2.27845591e-02 -5.36433347e-02\n",
      "  1.90500394e-02 -3.42503563e-02  1.32688815e-02 -5.41325193e-03\n",
      "  7.49740377e-03 -7.37037801e-04 -3.08569558e-02  3.82287689e-02\n",
      " -2.08311900e-02 -3.43154892e-02  5.60237328e-03  1.44999940e-02\n",
      " -3.76364030e-02 -5.11782058e-02 -3.51075381e-02  1.71867963e-02\n",
      "  1.50721688e-02 -9.62025896e-02 -1.53545542e-02  1.58375893e-02\n",
      "  2.42941042e-07 -5.88813331e-03  7.68795311e-02  5.86063452e-02\n",
      "  2.21232902e-02 -2.36690361e-02  5.25274873e-02  1.48660913e-02\n",
      "  7.34178722e-03 -4.98912809e-03  4.37413678e-02 -1.28331603e-02\n",
      "  3.37342322e-02 -1.10813929e-02 -1.33937774e-02 -7.80063719e-02\n",
      " -1.36330593e-02  1.94749273e-02  1.91754184e-03 -3.00251823e-02\n",
      "  1.02605642e-04  9.54534188e-02  1.19653895e-01  3.73371467e-02\n",
      "  4.25128080e-03  2.05130149e-02 -3.85415331e-02 -1.90614238e-02\n",
      "  5.88793531e-02  6.81264922e-02 -3.12596038e-02 -6.50440902e-02\n",
      "  2.48045567e-02  3.90002417e-04  7.54762441e-02 -3.46076526e-02\n",
      "  1.32949213e-02  4.14005630e-02  3.07568777e-02  5.50353341e-03\n",
      " -1.53084286e-03  2.75993180e-02  6.46033091e-03  1.05398316e-02\n",
      " -3.09298690e-02  4.60232869e-02 -3.64922099e-02 -1.39540350e-02\n",
      " -3.53720523e-02  7.97885412e-04  1.40632801e-02  1.80259235e-02\n",
      " -1.43368198e-02  2.19208188e-03 -3.96873914e-02 -1.17281917e-02\n",
      " -4.45220321e-02  8.05765111e-03 -4.04861383e-02  3.56149375e-02\n",
      "  5.12852408e-02 -6.64039403e-02 -5.32594845e-02  8.92902724e-03\n",
      "  1.56424902e-02  1.02110855e-01  8.10775254e-03 -4.03859606e-03\n",
      "  2.02352582e-34 -1.38294557e-02 -1.17623564e-02  1.51006728e-02\n",
      "  8.25895891e-02  2.39227936e-02 -1.10377725e-02  3.65657965e-03\n",
      " -7.44783645e-03  2.94555388e-02  3.52993980e-03 -6.10421300e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path = \"all-mpnet-base-v2\",\n",
    "                                      device=\"mps\")\n",
    "\n",
    "# Create a list of sentences to turn into numbers\n",
    "sentences = [\n",
    "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
    "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
    "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
    "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_model.encode(\"My favourite animal is the cow\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "#     item[\"embedding\"]=embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69 μs, sys: 20 μs, total: 89 μs\n",
      "Wall time: 90.8 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Using the above information, the student can find the optimum orientation point. The student does the procedure by slowly moving and stopping the mind's eye within the general area of the existing orientation point. This is done until perfect balance is achieved, and he or she experiences an overall feeling of well-being. Fine Tuning Procedure As in all these procedures, use your own words.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "text_chunks[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#Embed all texts into batches\n",
    "text_chunk_embedding  = embedding_model.encode(text_chunks , \n",
    "                                               batch_size =32 ,\n",
    "                                               convert_to_tensors= True)\n",
    "\n",
    "text_chunk_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings to file\n",
    "\n",
    "Since creating embeddings can be a timely process (not so much for our case but it can be for more larger datasets), let's turn our `pages_and_chunks_over_min_token_len` list of dictionaries into a DataFrame and save it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search is basically the embedding comparison\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "\n",
    "#import texts and embedding df\n",
    "text_chunks_and_embedding_df =pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#convert text and embeddings into list of dict\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient = \"records\")\n",
    "\n",
    "# Convert string embeddings to NumPy arrays\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(\n",
    "    lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \", dtype=np.float32)\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensor and move to device\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "\n",
    "print(embeddings.shape)  # Check if conversion was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) # choose the device to load the model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the query\n",
    "# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n",
    "query = \"macronutrients functions\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples \n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open PDF and load target page\n",
    "pdf_path = \"Gift_of_Dyslexia.pdf\" # requires PDF to be downloaded\n",
    "doc = fitz.open(pdf_path)\n",
    "page = doc.load_page(5 + 41) # number of page (our doc starts page numbers on page 41)\n",
    "\n",
    "# Get the image of the page\n",
    "img = page.get_pixmap(dpi=300)\n",
    "\n",
    "# Optional: save the image\n",
    "#img.save(\"output_filename.png\")\n",
    "doc.close()\n",
    "\n",
    "# Convert the Pixmap to a numpy array\n",
    "img_array = np.frombuffer(img.samples_mv, \n",
    "                          dtype=np.uint8).reshape((img.h, img.w, img.n))\n",
    "\n",
    "# Display the image using Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(13, 10))\n",
    "plt.imshow(img_array)\n",
    "plt.title(f\"Query: '{query}' | Most relevant page:\")\n",
    "plt.axis('off') # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "    return torch.dot(vector1, vector2)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = torch.dot(vector1, vector2)\n",
    "\n",
    "    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)\n",
    "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
    "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
    "\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "# Example tensors\n",
    "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
    "\n",
    "# Calculate dot product\n",
    "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
    "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
    "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
    "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
    "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionizing our semantic search pipeline\n",
    "\n",
    "Let's put all of the steps from above for semantic search into a function or two so we can repeat the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"symptoms of pellagra\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking local GPU memory availability\n",
    "\n",
    "Let's find out what hardware we've got available and see what kind of model(s) we'll be able to load.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "# Get system memory (Apple M-Series shares memory with CPU)\n",
    "total_memory_gb = round(psutil.virtual_memory().total / (2**30), 2)\n",
    "\n",
    "print(f\"Total system memory: {total_memory_gb} GB (Shared between CPU & GPU)\")\n",
    "\n",
    "# Torch MPS does not expose memory details like CUDA\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is available. Memory is dynamically allocated.\")\n",
    "else:\n",
    "    print(\"MPS backend is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Select Gemma model based on available GPU memory\n",
    "if total_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {total_memory_gb:.2f}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "    use_quantization_config = True\n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif total_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {total_memory_gb:.2f}GB | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif total_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {total_memory_gb:.2f}GB | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "else:  # total_memory_gb >= 19.0\n",
    "    print(f\"GPU memory: {total_memory_gb:.2f}GB | Recommended model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "# Determine device and best attention mechanism\n",
    "def get_device_and_attention():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        if is_flash_attn_2_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
    "            attn_implementation = \"flash_attention_2\"\n",
    "        else:\n",
    "            attn_implementation = \"sdpa\"\n",
    "        print(\"[INFO] Using CUDA with:\", attn_implementation)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        attn_implementation = \"sdpa\"  # Best available for MPS\n",
    "        print(\"[INFO] Using MPS with SDPA\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        attn_implementation = None  # No special attention on CPU\n",
    "        print(\"[WARNING] Using CPU, expect slower performance\")\n",
    "    \n",
    "    return device, attn_implementation\n",
    "\n",
    "# Get device and attention mechanism\n",
    "device, attn_implementation = get_device_and_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == \"mps\":\n",
    "    torch_dtype = torch.float32\n",
    "else:\n",
    "    torch_dtype = torch.bfloat16\n",
    "\n",
    "# Load tokenizer and model with the appropriate settings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id , token = \"\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    # quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=False,\n",
    "    attn_implementation=attn_implementation,\n",
    "    token=\"\"\n",
    ")\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU \n",
    "    model.to(\"mps\")\n",
    "print(\"[INFO] Model successfully loaded on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, ok a bunch of layers ranging from embedding layers to attention layers (see the `GemmaFlashAttention2` layers!) to MLP and normalization layers.\n",
    "\n",
    "The good news is that we don't have to know too much about these to use the model.\n",
    "\n",
    "How about we get the number of parameters in our model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig \n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)  # define the maximum number of new tokens to create\n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! We just generated some text on our local GPU!\n",
    "\n",
    "Well not just yet...\n",
    "\n",
    "Our LLM accepts tokens in and sends tokens back out.\n",
    "\n",
    "We can conver the output tokens to text using [`tokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input text: {input_text}\\n\")\n",
    "print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutrition-style questions generated with GPT4\n",
    "/gpt4_questions = [\n",
    "    \"What are the macronutrients, and what roles do they play in the human body?\",\n",
    "    \"How do vitamins and minerals differ in their roles and importance for health?\",\n",
    "    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
    "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
    "    \"Explain the concept of energy balance and its importance in weight management.\"\n",
    "]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"How often should infants be breastfed?\",\n",
    "    \"What are symptoms of pellagra?\",\n",
    "    \"How does saliva help with digestion?\",\n",
    "    \"What is the RDI for protein per day?\",\n",
    "    \"water soluble vitamins\"\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check if our `retrieve_relevant_resources()` function works with our list of queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "query = random.choice(query_list)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prompt_formatter(query: str, \n",
    "#                      context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are the fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "\\nExample 2:\n",
    "Query: What are the causes of type 2 diabetes?\n",
    "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "\\nExample 3:\n",
    "Query: What is the importance of hydration for physical performance?\n",
    "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "    \n",
    "# Create a list of context items\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = model.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt \n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesssssss!!!\n",
    "\n",
    "Our RAG pipeline is complete!\n",
    "\n",
    "We just Retrieved, Augmented and Generated!\n",
    "\n",
    "And all on our own local GPU!\n",
    "\n",
    "How about we functionize the generation step to make it easier to use?\n",
    "\n",
    "We can put a little formatting on the text being returned to make it look nice too.\n",
    "\n",
    "And we'll make an option to return the context items if needed as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "    \n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local RAG workflow complete!\n",
    "\n",
    "We've now officially got a way to Retrieve, Augment and Generate answers based on a source.\n",
    "\n",
    "For now we can verify our answers manually by reading them and reading through the textbook.\n",
    "\n",
    "But if you want to put this into a production system, it'd be a good idea to have some kind of evaluation on how well our pipeline works.\n",
    "\n",
    "For example, you could use another LLM to rate the answers returned by our LLM and then use those ratings as a proxy evaluation.\n",
    "\n",
    "However, I'll leave this and a few more interesting ideas as extensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
